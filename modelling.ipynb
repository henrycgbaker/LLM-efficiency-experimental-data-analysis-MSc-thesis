{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb803c01",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- [ ] WORK OUT % OF TOTAL ENERGY CONSUMPTION / VARIANCE THAT CAN BE EXPLAINED BY TOTAL CONFIGS CHANGES VS BASELINE THAT IS MODEL DETERMINED\n",
    "- [ ] set up hoerarchical model -> identify groupings:\n",
    "   - [ ] use model (even though currently only one model)\n",
    "   - [ ] use config_name and date_time\n",
    "   - [ ] use decoder configs\n",
    "   - [ ] latency configs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66eebd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total generated tokens value is constant: 16384\n",
      "Original distribution:\n",
      "total_generated_tokens\n",
      "16384    2682\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Round 1: Verfifying FLOPs on raw df\n",
      "NB: FLOPs values are NOT constant: [1.69499710e+13 2.02486233e+13 0.00000000e+00 5.26385823e+13]\n",
      "Original distribution:\n",
      "flops\n",
      "0.000000e+00       5\n",
      "1.694997e+13      22\n",
      "2.024862e+13    2265\n",
      "5.263858e+13     390\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dominant FLOPs value: 20248623316992.0\n",
      "- Affected rows count: 417\n",
      "- Affected row indices: [0, 36, 155, 234, 283, 426, 548, 617, 630, 683, 719, 761, 904, 1000, 1051, 1246, 1294, 1392, 1485, 1531, 1631, 1779, 1815, 2038, 2071, 2214, 2239, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681]\n",
      "- Affected configs: ['precis_float16_quant_True_quant8_True_quant_False', 'precis_float16_quant_True_quant_False_quant4_True', 'batching_64', 'decoding_top_p_topp_0.1_temp_1.2', 'decoding_top_k_topk_300_temp_0.6', 'decoding_top_k_topk_1_temp_1.2', 'decoding_greedy_temp_0.8', 'latency_burst_0.4_0.5_4.0_20', 'latency_burst_0.1_0.2_6.0_5', 'num_processes_2', 'decoding_top_p_topp_0.3_temp_1.0', 'precis_float16_quant_False_quant_False_quant_False', 'decoding_top_p_topp_0.98_temp_1.4', 'num_processes_4', 'batching_24', 'latency_burst_0.2_0.4_4.0_10', 'decoding_top_k_topk_500_temp_0.6', 'precis_float32_quant_False_quant_False_quant_False', 'latency_burst_0.05_0.1_4.0_20', 'latency_burst_0.4_0.5_2.0_20', 'latency_burst_0.1_0.2_2.0_20', 'decoding_top_p_topp_0.1_temp_0.6', 'batching_3', 'batching_14', 'decoding_top_p_topp_0.1_temp_1.4', 'batching_12', 'decoding_top_k_topk_400_temp_1.4', 'latency_burst_0.2_0.4_4.0_20', 'batching_1', 'batching_10', 'decoding_top_p_topp_0.7_temp_0.6', 'decoding_top_k_topk_1_temp_0.4', 'latency_burst_0.05_0.1_4.0_5', 'latency_burst_0.05_0.1_4.0_10', 'decoding_top_p_topp_0.8_temp_1.4', 'decoding_top_k_topk_20_temp_1.2', 'latency_burst_0.1_0.2_6.0_8', 'decoding_top_k_topk_500_temp_0.4', 'latency_burst_0.4_0.5_2.0_8', 'decoding_top_p_topp_0.9_temp_0.2', 'decoding_top_k_topk_1_temp_0.0', 'decoding_top_p_topp_0.98_temp_0.0', 'decoding_top_p_topp_0.3_temp_0.0', 'decoding_top_k_topk_1_temp_0.2', 'decoding_top_p_topp_0.9_temp_1.4', 'decoding_top_k_topk_50_temp_1.2', 'latency_burst_0.4_0.5_2.0_10', 'latency_burst_0.05_0.1_4.0_8', 'decoding_top_k_topk_100_temp_0.0', 'decoding_top_k_topk_5_temp_0.4', 'latency_burst_0.05_0.1_6.0_10', 'decoding_greedy_temp_0.2', 'decoding_top_p_topp_0.3_temp_0.6', 'decoding_top_k_topk_20_temp_0.6', 'decoding_top_k_topk_400_temp_0.0', 'latency_burst_0.05_0.1_2.0_5', 'latency_burst_0.4_0.5_6.0_5', 'decoding_top_k_topk_5_temp_0.6', 'latency_burst_0.2_0.4_2.0_8', 'latency_const_0.05_0.1', 'batching_48', 'decoding_top_k_topk_5_temp_1.2', 'latency_burst_0.2_0.4_6.0_5', 'latency_const_0.2_0.4', 'batching_4', 'latency_burst_0.2_0.4_4.0_8', 'latency_burst_0.4_0.5_4.0_5', 'latency_burst_0.1_0.2_6.0_10', 'decoding_top_p_topp_0.98_temp_1.2', 'decoding_top_k_topk_100_temp_0.8', 'batching_6', 'decoding_top_p_topp_0.3_temp_1.4', 'decoding_top_k_topk_400_temp_0.2', 'decoding_top_k_topk_5_temp_0.8', 'latency_burst_0.05_0.1_2.0_20', 'batching_5', 'decoding_greedy_temp_1.0', 'decoding_greedy_temp_1.4', 'decoding_top_k_topk_50_temp_0.4', 'decoding_top_p_topp_0.3_temp_0.2', 'latency_burst_0.2_0.4_6.0_20', 'latency_burst_0.1_0.2_4.0_10', 'decoding_top_k_topk_400_temp_1.0', 'decoding_top_k_topk_500_temp_1.0', 'latency_const_0.4_0.5', 'decoding_top_p_topp_0.5_temp_1.0', 'batching_8', 'num_processes_3', 'decoding_top_k_topk_200_temp_0.2', 'latency_burst_0.2_0.4_2.0_20', 'decoding_top_p_topp_0.9_temp_1.2', 'decoding_top_p_topp_0.3_temp_1.2', 'decoding_top_k_topk_500_temp_0.2', 'decoding_top_p_topp_0.9_temp_0.6', 'decoding_top_p_topp_0.9_temp_1.0', 'decoding_top_k_topk_1_temp_0.6', 'decoding_top_k_topk_400_temp_0.4', 'latency_burst_0.05_0.1_6.0_20', 'decoding_top_k_topk_200_temp_0.8', 'decoding_top_k_topk_400_temp_0.8', 'decoding_top_p_topp_0.5_temp_0.4', 'decoding_top_k_topk_500_temp_0.0', 'latency_burst_0.05_0.1_6.0_8', 'latency_burst_0.05_0.1_2.0_10', 'decoding_top_p_topp_0.1_temp_0.0', 'latency_burst_0.4_0.5_6.0_20', 'latency_burst_0.1_0.2_2.0_8', 'decoding_top_k_topk_500_temp_0.8', 'decoding_top_p_topp_0.8_temp_1.2', 'latency_burst_0.1_0.2_4.0_5', 'decoding_top_k_topk_20_temp_0.4', 'decoding_top_p_topp_0.9_temp_0.0', 'latency_burst_0.05_0.1_2.0_8', 'decoding_top_p_topp_0.9_temp_0.4', 'decoding_top_k_topk_500_temp_1.2', 'decoding_top_k_topk_300_temp_0.0', 'decoding_top_p_topp_0.1_temp_1.0', 'decoding_top_k_topk_200_temp_0.4', 'decoding_top_k_topk_200_temp_1.4', 'decoding_top_k_topk_5_temp_0.0', 'decoding_top_p_topp_0.5_temp_0.6', 'decoding_greedy_temp_0.4', 'decoding_top_k_topk_20_temp_0.0', 'batching_16', 'decoding_greedy_temp_0.6', 'decoding_top_k_topk_20_temp_0.2', 'decoding_top_k_topk_5_temp_1.0', 'decoding_top_p_topp_0.7_temp_0.2', 'decoding_top_p_topp_0.7_temp_1.2', 'decoding_top_p_topp_0.98_temp_0.8', 'decoding_top_p_topp_0.5_temp_1.2', 'decoding_top_p_topp_0.98_temp_1.0', 'decoding_greedy_temp_0.0', 'decoding_top_p_topp_0.3_temp_0.8', 'decoding_top_p_topp_0.1_temp_0.8', 'decoding_top_k_topk_50_temp_0.0', 'decoding_top_k_topk_300_temp_1.2', 'decoding_top_p_topp_0.8_temp_1.0', 'decoding_top_p_topp_0.98_temp_0.6', 'latency_burst_0.4_0.5_2.0_5', 'decoding_top_k_topk_200_temp_0.0', 'batching_20', 'decoding_top_k_topk_100_temp_1.4', 'decoding_top_k_topk_300_temp_1.4', 'num_processes_1', 'decoding_top_p_topp_0.7_temp_1.4', 'latency_burst_0.2_0.4_2.0_10', 'decoding_top_k_topk_20_temp_1.4', 'decoding_top_p_topp_0.7_temp_1.0', 'latency_burst_0.4_0.5_4.0_10', 'decoding_top_p_topp_0.8_temp_0.6', 'decoding_top_k_topk_50_temp_0.6', 'decoding_top_p_topp_0.8_temp_0.8', 'decoding_top_p_topp_0.1_temp_0.2', 'decoding_top_p_topp_0.7_temp_0.8', 'latency_burst_0.2_0.4_6.0_10', 'latency_burst_0.05_0.1_6.0_5', 'decoding_top_k_topk_300_temp_0.2', 'latency_burst_0.2_0.4_4.0_5', 'batching_28', 'batching_2', 'decoding_top_k_topk_500_temp_1.4', 'decoding_top_p_topp_0.8_temp_0.4', 'latency_burst_0.1_0.2_6.0_20', 'decoding_top_k_topk_200_temp_1.0', 'decoding_top_p_topp_0.5_temp_0.8', 'batching_32', 'decoding_top_p_topp_0.7_temp_0.4', 'decoding_top_k_topk_50_temp_1.0', 'decoding_top_k_topk_5_temp_0.2', 'decoding_top_p_topp_0.5_temp_0.2', 'decoding_top_p_topp_0.3_temp_0.4', 'decoding_top_k_topk_100_temp_0.6', 'latency_burst_0.1_0.2_4.0_8', 'latency_burst_0.2_0.4_2.0_5', 'batching_56', 'decoding_top_k_topk_20_temp_0.8', 'latency_off', 'decoding_top_k_topk_300_temp_1.0', 'latency_burst_0.4_0.5_4.0_8', 'decoding_top_p_topp_0.9_temp_0.8', 'latency_burst_0.4_0.5_6.0_8', 'decoding_top_p_topp_0.5_temp_0.0', 'decoding_top_k_topk_200_temp_0.6', 'latency_burst_0.1_0.2_2.0_5', 'decoding_top_p_topp_0.98_temp_0.4', 'decoding_top_k_topk_100_temp_1.0', 'latency_burst_0.2_0.4_6.0_8', 'batching_40', 'decoding_top_k_topk_400_temp_0.6', 'decoding_top_k_topk_300_temp_0.4', 'decoding_top_k_topk_1_temp_1.0', 'decoding_top_k_topk_5_temp_1.4', 'decoding_top_k_topk_50_temp_0.2', 'decoding_top_k_topk_100_temp_0.2', 'latency_burst_0.1_0.2_4.0_20', 'decoding_top_k_topk_50_temp_0.8', 'decoding_greedy_temp_1.2', 'decoding_top_k_topk_200_temp_1.2', 'decoding_top_p_topp_0.5_temp_1.4', 'decoding_top_k_topk_50_temp_1.4', 'latency_burst_0.1_0.2_2.0_10', 'decoding_top_p_topp_0.98_temp_0.2', 'decoding_top_k_topk_100_temp_1.2', 'decoding_top_k_topk_100_temp_0.4', 'decoding_top_k_topk_1_temp_0.8', 'latency_burst_0.4_0.5_6.0_10', 'decoding_top_p_topp_0.1_temp_0.4', 'decoding_top_k_topk_400_temp_1.2', 'decoding_top_p_topp_0.7_temp_0.0', 'decoding_top_p_topp_0.8_temp_0.2', 'decoding_top_p_topp_0.8_temp_0.0', 'decoding_top_k_topk_1_temp_1.4', 'decoding_top_k_topk_300_temp_0.8', 'decoding_top_k_topk_20_temp_1.0', 'latency_const_0.1_0.2']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "FLOP Differentiators:\n",
      "model: {np.float64(16949970993152.0): 'meta-llama/Llama-3.2-1B', np.float64(20248623316992.0): 'meta-llama/Llama-3.2-1B', np.float64(0.0): 'meta-llama/Llama-3.2-1B', np.float64(52638582308864.0): 'meta-llama/Llama-3.2-3B'}\n",
      "Round 2: Verfifying FLOPs on corrected df\n",
      "NB: FLOPs values are NOT constant: [1.69499710e+13 5.26385823e+13]\n",
      "Original distribution:\n",
      "flops\n",
      "1.694997e+13    2292\n",
      "5.263858e+13     390\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dominant FLOPs value: 16949970993152.0\n",
      "- Affected rows count: 390\n",
      "- Affected row indices: [2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681]\n",
      "- Affected configs: ['decoding_top_p_topp_0.1_temp_1.2', 'decoding_top_k_topk_300_temp_0.6', 'decoding_top_k_topk_1_temp_1.2', 'decoding_greedy_temp_0.8', 'latency_burst_0.4_0.5_4.0_20', 'latency_burst_0.1_0.2_6.0_5', 'num_processes_2', 'decoding_top_p_topp_0.3_temp_1.0', 'precis_float16_quant_False_quant_False_quant_False', 'decoding_top_p_topp_0.98_temp_1.4', 'num_processes_4', 'batching_24', 'latency_burst_0.2_0.4_4.0_10', 'decoding_top_k_topk_500_temp_0.6', 'precis_float32_quant_False_quant_False_quant_False', 'latency_burst_0.05_0.1_4.0_20', 'latency_burst_0.4_0.5_2.0_20', 'latency_burst_0.1_0.2_2.0_20', 'decoding_top_p_topp_0.1_temp_0.6', 'batching_3', 'batching_14', 'decoding_top_p_topp_0.1_temp_1.4', 'batching_12', 'decoding_top_k_topk_400_temp_1.4', 'latency_burst_0.2_0.4_4.0_20', 'batching_1', 'batching_10', 'decoding_top_p_topp_0.7_temp_0.6', 'decoding_top_k_topk_1_temp_0.4', 'latency_burst_0.05_0.1_4.0_5', 'latency_burst_0.05_0.1_4.0_10', 'decoding_top_p_topp_0.8_temp_1.4', 'decoding_top_k_topk_20_temp_1.2', 'latency_burst_0.1_0.2_6.0_8', 'decoding_top_k_topk_500_temp_0.4', 'batching_64', 'latency_burst_0.4_0.5_2.0_8', 'decoding_top_p_topp_0.9_temp_0.2', 'decoding_top_k_topk_1_temp_0.0', 'decoding_top_p_topp_0.98_temp_0.0', 'decoding_top_p_topp_0.3_temp_0.0', 'decoding_top_k_topk_1_temp_0.2', 'decoding_top_p_topp_0.9_temp_1.4', 'decoding_top_k_topk_50_temp_1.2', 'latency_burst_0.4_0.5_2.0_10', 'latency_burst_0.05_0.1_4.0_8', 'decoding_top_k_topk_100_temp_0.0', 'decoding_top_k_topk_5_temp_0.4', 'latency_burst_0.05_0.1_6.0_10', 'decoding_greedy_temp_0.2', 'decoding_top_p_topp_0.3_temp_0.6', 'decoding_top_k_topk_20_temp_0.6', 'decoding_top_k_topk_400_temp_0.0', 'latency_burst_0.05_0.1_2.0_5', 'latency_burst_0.4_0.5_6.0_5', 'decoding_top_k_topk_5_temp_0.6', 'latency_burst_0.2_0.4_2.0_8', 'latency_const_0.05_0.1', 'batching_48', 'decoding_top_k_topk_5_temp_1.2', 'latency_burst_0.2_0.4_6.0_5', 'latency_const_0.2_0.4', 'batching_4', 'latency_burst_0.2_0.4_4.0_8', 'latency_burst_0.4_0.5_4.0_5', 'latency_burst_0.1_0.2_6.0_10', 'decoding_top_p_topp_0.98_temp_1.2', 'decoding_top_k_topk_100_temp_0.8', 'batching_6', 'decoding_top_p_topp_0.3_temp_1.4', 'decoding_top_k_topk_400_temp_0.2', 'decoding_top_k_topk_5_temp_0.8', 'latency_burst_0.05_0.1_2.0_20', 'batching_5', 'decoding_greedy_temp_1.0', 'decoding_greedy_temp_1.4', 'decoding_top_k_topk_50_temp_0.4', 'decoding_top_p_topp_0.3_temp_0.2', 'latency_burst_0.2_0.4_6.0_20', 'latency_burst_0.1_0.2_4.0_10', 'decoding_top_k_topk_400_temp_1.0', 'decoding_top_k_topk_500_temp_1.0', 'latency_const_0.4_0.5', 'decoding_top_p_topp_0.5_temp_1.0', 'batching_8', 'num_processes_3', 'decoding_top_k_topk_200_temp_0.2', 'latency_burst_0.2_0.4_2.0_20', 'decoding_top_p_topp_0.9_temp_1.2', 'decoding_top_p_topp_0.3_temp_1.2', 'decoding_top_k_topk_500_temp_0.2', 'decoding_top_p_topp_0.9_temp_0.6', 'decoding_top_p_topp_0.9_temp_1.0', 'decoding_top_k_topk_1_temp_0.6', 'decoding_top_k_topk_400_temp_0.4', 'latency_burst_0.05_0.1_6.0_20', 'decoding_top_k_topk_200_temp_0.8', 'decoding_top_k_topk_400_temp_0.8', 'decoding_top_p_topp_0.5_temp_0.4', 'decoding_top_k_topk_500_temp_0.0', 'latency_burst_0.05_0.1_6.0_8', 'latency_burst_0.05_0.1_2.0_10', 'decoding_top_p_topp_0.1_temp_0.0', 'latency_burst_0.4_0.5_6.0_20', 'latency_burst_0.1_0.2_2.0_8', 'decoding_top_k_topk_500_temp_0.8', 'decoding_top_p_topp_0.8_temp_1.2', 'latency_burst_0.1_0.2_4.0_5', 'decoding_top_k_topk_20_temp_0.4', 'decoding_top_p_topp_0.9_temp_0.0', 'latency_burst_0.05_0.1_2.0_8', 'decoding_top_p_topp_0.9_temp_0.4', 'decoding_top_k_topk_500_temp_1.2', 'decoding_top_k_topk_300_temp_0.0', 'decoding_top_p_topp_0.1_temp_1.0', 'decoding_top_k_topk_200_temp_0.4', 'decoding_top_k_topk_200_temp_1.4', 'decoding_top_k_topk_5_temp_0.0', 'decoding_top_p_topp_0.5_temp_0.6', 'decoding_greedy_temp_0.4', 'decoding_top_k_topk_20_temp_0.0', 'batching_16', 'decoding_greedy_temp_0.6', 'decoding_top_k_topk_20_temp_0.2', 'decoding_top_k_topk_5_temp_1.0', 'decoding_top_p_topp_0.7_temp_0.2', 'decoding_top_p_topp_0.7_temp_1.2', 'decoding_top_p_topp_0.98_temp_0.8', 'decoding_top_p_topp_0.5_temp_1.2', 'precis_float16_quant_True_quant8_True_quant_False', 'decoding_top_p_topp_0.98_temp_1.0', 'decoding_greedy_temp_0.0', 'decoding_top_p_topp_0.3_temp_0.8', 'decoding_top_p_topp_0.1_temp_0.8', 'decoding_top_k_topk_50_temp_0.0', 'decoding_top_k_topk_300_temp_1.2', 'decoding_top_p_topp_0.8_temp_1.0', 'decoding_top_p_topp_0.98_temp_0.6', 'latency_burst_0.4_0.5_2.0_5', 'decoding_top_k_topk_200_temp_0.0', 'batching_20', 'decoding_top_k_topk_100_temp_1.4', 'decoding_top_k_topk_300_temp_1.4', 'num_processes_1', 'decoding_top_p_topp_0.7_temp_1.4', 'latency_burst_0.2_0.4_2.0_10', 'decoding_top_k_topk_20_temp_1.4', 'decoding_top_p_topp_0.7_temp_1.0', 'latency_burst_0.4_0.5_4.0_10', 'decoding_top_p_topp_0.8_temp_0.6', 'precis_float16_quant_True_quant_False_quant4_True', 'decoding_top_k_topk_50_temp_0.6', 'decoding_top_p_topp_0.8_temp_0.8', 'decoding_top_p_topp_0.1_temp_0.2', 'decoding_top_p_topp_0.7_temp_0.8', 'latency_burst_0.2_0.4_6.0_10', 'latency_burst_0.05_0.1_6.0_5', 'decoding_top_k_topk_300_temp_0.2', 'latency_burst_0.2_0.4_4.0_5', 'batching_28', 'batching_2', 'decoding_top_k_topk_500_temp_1.4', 'decoding_top_p_topp_0.8_temp_0.4', 'latency_burst_0.1_0.2_6.0_20', 'decoding_top_k_topk_200_temp_1.0', 'decoding_top_p_topp_0.5_temp_0.8', 'batching_32', 'decoding_top_p_topp_0.7_temp_0.4', 'decoding_top_k_topk_50_temp_1.0', 'decoding_top_k_topk_5_temp_0.2', 'decoding_top_p_topp_0.5_temp_0.2', 'decoding_top_p_topp_0.3_temp_0.4', 'decoding_top_k_topk_100_temp_0.6', 'latency_burst_0.1_0.2_4.0_8', 'latency_burst_0.2_0.4_2.0_5', 'batching_56', 'decoding_top_k_topk_20_temp_0.8', 'latency_off', 'decoding_top_k_topk_300_temp_1.0', 'latency_burst_0.4_0.5_4.0_8', 'decoding_top_p_topp_0.9_temp_0.8', 'latency_burst_0.4_0.5_6.0_8', 'decoding_top_p_topp_0.5_temp_0.0', 'decoding_top_k_topk_200_temp_0.6', 'latency_burst_0.1_0.2_2.0_5', 'decoding_top_p_topp_0.98_temp_0.4', 'decoding_top_k_topk_100_temp_1.0', 'latency_burst_0.2_0.4_6.0_8', 'batching_40', 'decoding_top_k_topk_400_temp_0.6', 'decoding_top_k_topk_300_temp_0.4', 'decoding_top_k_topk_1_temp_1.0', 'decoding_top_k_topk_5_temp_1.4', 'decoding_top_k_topk_50_temp_0.2', 'decoding_top_k_topk_100_temp_0.2', 'latency_burst_0.1_0.2_4.0_20', 'decoding_top_k_topk_50_temp_0.8', 'decoding_greedy_temp_1.2', 'decoding_top_k_topk_200_temp_1.2', 'decoding_top_p_topp_0.5_temp_1.4', 'decoding_top_k_topk_50_temp_1.4', 'latency_burst_0.1_0.2_2.0_10', 'decoding_top_p_topp_0.98_temp_0.2', 'decoding_top_k_topk_100_temp_1.2', 'decoding_top_k_topk_100_temp_0.4', 'decoding_top_k_topk_1_temp_0.8', 'latency_burst_0.4_0.5_6.0_10', 'decoding_top_p_topp_0.1_temp_0.4', 'decoding_top_k_topk_400_temp_1.2', 'decoding_top_p_topp_0.7_temp_0.0', 'decoding_top_p_topp_0.8_temp_0.2', 'decoding_top_p_topp_0.8_temp_0.0', 'decoding_top_k_topk_1_temp_1.4', 'decoding_top_k_topk_300_temp_0.8', 'decoding_top_k_topk_20_temp_1.0', 'latency_const_0.1_0.2']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✅ FLOPs are unique per model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cyles present: [ 1  2  3  4  5  6  7  8  9 10 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrybaker/repositories/thesis_analysis/scripts/a_data_loading_cleaning.py:611: UserWarning: NB: FLOPs values are NOT constant: [1.69499710e+13 2.02486233e+13 0.00000000e+00 5.26385823e+13]\n",
      "  if not verify_flops(df):\n",
      "/Users/henrybaker/repositories/thesis_analysis/scripts/a_data_loading_cleaning.py:621: UserWarning: NB: FLOPs values are NOT constant: [1.69499710e+13 5.26385823e+13]\n",
      "  verify_flops(df)\n"
     ]
    }
   ],
   "source": [
    "from scripts.a_data_loading_cleaning import run_load_clean_diagnose_data\n",
    "\n",
    "csv_path = f\"results/controlled_results.csv\"\n",
    "df = run_load_clean_diagnose_data(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89db3fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['config_name', 'experiment_id', 'cycle_id', 'date_time', 'model',\n",
       "       'num_processes', 'batch_size___fixed_batching', 'decoder_temperature',\n",
       "       'decoder_top_k', 'decoder_top_p', 'latency_simulation_simulate',\n",
       "       'latency_simulation_delay_max', 'latency_simulation_delay_min',\n",
       "       'latency_simulation_simulate_burst', 'latency_simulation_burst_size',\n",
       "       'latency_simulation_burst_interval', 'fp_precision', 'quantization',\n",
       "       'load_in_8bit', 'load_in_4bit', 'total_input_tokens',\n",
       "       'total_generated_tokens', 'date_time', 'total_params',\n",
       "       'max_input_tokens', 'max_output_tokens', 'number_input_prompts',\n",
       "       'total_energy_kwh', 'total_energy_joules', 'flops', 'tokens_per_joule',\n",
       "       'joules_per_token', 'flops_per_joule', 'joules_per_flop',\n",
       "       'total_inference_time_sec', 'average_latency_ms_per_batch',\n",
       "       'throughput_queries_per_sec', 'throughput_tokens_per_sec',\n",
       "       'gpu_utilization_percent_0', 'gpu_utilization_percent_1',\n",
       "       'gpu_utilization_percent_2', 'gpu_utilization_percent_3',\n",
       "       'gpu_power_process_0', 'gpu_power_process_1', 'gpu_power_process_2',\n",
       "       'gpu_power_process_3', 'gpu_energy_process_0', 'gpu_energy_process_1',\n",
       "       'gpu_energy_process_2', 'gpu_energy_process_3', 'ram_energy_process_0',\n",
       "       'ram_energy_process_1', 'ram_energy_process_2', 'ram_energy_process_3',\n",
       "       'total_energy_kwh_process_0', 'total_energy_kwh_process_1',\n",
       "       'total_energy_kwh_process_2', 'total_energy_kwh_process_3',\n",
       "       'gpu_power_avg', 'ram_power_avg', 'cpu_energy_total',\n",
       "       'gpu_energy_total', 'decoder_config_decoding_mode', 'models',\n",
       "       'flops_per_token', 'energy_per_token_kwh', 'divergence_energy_flops',\n",
       "       'gpu_utilization_proc_all', 'gpu_power_proc_all',\n",
       "       'total_energy_kwh_mean', 'total_energy_kwh_std',\n",
       "       'total_inference_time_sec_mean', 'total_inference_time_sec_std',\n",
       "       'average_latency_ms_per_batch_mean', 'average_latency_ms_per_batch_std',\n",
       "       'throughput_queries_per_sec_mean', 'throughput_queries_per_sec_std',\n",
       "       'throughput_tokens_per_sec_mean', 'throughput_tokens_per_sec_std',\n",
       "       'cpu_energy_total_mean', 'cpu_energy_total_std',\n",
       "       'gpu_energy_total_mean', 'gpu_energy_total_std', 'flops_mean',\n",
       "       'flops_std', 'flops_per_token_mean', 'flops_per_token_std',\n",
       "       'energy_per_token_kwh_mean', 'energy_per_token_kwh_std',\n",
       "       'divergence_energy_flops_mean', 'divergence_energy_flops_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = df.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f74e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1687, 8)\n",
      "Rank : 5\n",
      "Cond : 1.97255664858682e+20\n",
      "                    variable       VIF\n",
      "1  C(decoding_mode)[T.top_k]  5.188534\n",
      "2  C(decoding_mode)[T.top_p]  4.715084\n",
      "7             sampling_param  1.470129\n",
      "6        decoder_temperature  1.000182\n",
      "0                  Intercept  0.000000\n",
      "3         batch_size_numeric  0.000000\n",
      "4              num_processes  0.000000\n",
      "5             precision_bits  0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrybaker/miniconda3/envs/thesis/lib/python3.12/site-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 1 - self.ssr/self.centered_tss\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# 1) Copy and create your derived columns\n",
    "df2 = df.copy()\n",
    "\n",
    "# 1a) Numeric batch size\n",
    "df2['batch_size_numeric'] = pd.to_numeric(\n",
    "    df2['batch_size___fixed_batching'], errors='coerce'\n",
    ")\n",
    "\n",
    "# 1b) Collapse precision levels into one ordinal variable\n",
    "df2['precision_bits'] = 32\n",
    "df2.loc[df2['fp_precision']=='torch.float16',     'precision_bits'] = 16\n",
    "df2.loc[df2['load_in_8bit']         == True,      'precision_bits'] = 8\n",
    "df2.loc[df2['load_in_4bit']         == True,      'precision_bits'] = 4\n",
    "\n",
    "# 1c) Collapse decoder mode + param\n",
    "df2['decoding_mode']   = df2['decoder_config_decoding_mode'].astype('category')\n",
    "df2['sampling_param']  = 0.0\n",
    "mask_k = df2['decoding_mode']=='top_k'\n",
    "mask_p = df2['decoding_mode']=='top_p'\n",
    "df2.loc[mask_k, 'sampling_param'] = df2.loc[mask_k, 'decoder_top_k']\n",
    "df2.loc[mask_p, 'sampling_param'] = df2.loc[mask_p, 'decoder_top_p']\n",
    "\n",
    "# 1d) Drop the old columns you no longer need\n",
    "df2 = df2.drop(columns=[\n",
    "    'fp_precision','quantization','load_in_8bit','load_in_4bit',\n",
    "    'decoder_config_decoding_mode','decoder_top_k','decoder_top_p',\n",
    "    'batch_size___fixed_batching'\n",
    "])\n",
    "\n",
    "# 2) Define which columns must be present (no NAs) for the VIF check\n",
    "keep = [\n",
    "    'energy_per_token_kwh','batch_size_numeric','num_processes',\n",
    "    'precision_bits','decoder_temperature','sampling_param',\n",
    "    'decoding_mode'\n",
    "]\n",
    "\n",
    "# 3) Drop rows where any of these is missing\n",
    "df3 = df2.dropna(subset=keep)\n",
    "\n",
    "# 4) Build and inspect the design matrix\n",
    "formula = (\n",
    "    \"energy_per_token_kwh ~ batch_size_numeric + num_processes + \"\n",
    "    \"precision_bits + decoder_temperature + sampling_param + \"\n",
    "    \"C(decoding_mode)\"\n",
    ")\n",
    "y, X = patsy.dmatrices(formula, df3, return_type='dataframe')\n",
    "\n",
    "print(\"Shape:\", X.shape)\n",
    "print(\"Rank :\", np.linalg.matrix_rank(X.values))\n",
    "print(\"Cond :\", np.linalg.cond(X.values))\n",
    "\n",
    "# 5) Compute VIFs\n",
    "vif = pd.DataFrame({\n",
    "    'variable': X.columns,\n",
    "    'VIF': [variance_inflation_factor(X.values, i)\n",
    "            for i in range(X.shape[1])]\n",
    "}).sort_values('VIF', ascending=False)\n",
    "\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ee65d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post‐scaling condition number: 3.6859591916646165e+34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# columns to standardize\n",
    "num_cols = ['batch_size_numeric','num_processes','precision_bits',\n",
    "            'decoder_temperature','sampling_param']\n",
    "\n",
    "X_std = X.copy()\n",
    "X_std[num_cols] = StandardScaler().fit_transform(X[num_cols])\n",
    "\n",
    "print(\"Post‐scaling condition number:\", np.linalg.cond(X_std.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa31e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null‐space vector #1:\n",
      "  -0.999 × Intercept\n",
      "  +0.009 × batch_size_numeric\n",
      "  -0.032 × num_processes\n",
      "  +0.031 × precision_bits\n",
      "\n",
      "Null‐space vector #2:\n",
      "  +0.011 × Intercept\n",
      "  -0.877 × batch_size_numeric\n",
      "  -0.148 × num_processes\n",
      "  +0.457 × precision_bits\n",
      "\n",
      "Null‐space vector #3:\n",
      "  -0.035 × Intercept\n",
      "  -0.182 × batch_size_numeric\n",
      "  +0.982 × num_processes\n",
      "  -0.031 × precision_bits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# full SVD\n",
    "U, s, Vt = np.linalg.svd(X.values, full_matrices=False)\n",
    "\n",
    "# identify “zero” singular values\n",
    "tol = 1e-8\n",
    "null_idxs = np.where(s < tol)[0]\n",
    "\n",
    "# extract the corresponding right‐singular vectors\n",
    "nullspace = Vt[null_idxs, :]\n",
    "\n",
    "# display\n",
    "for i, v in enumerate(nullspace, 1):\n",
    "    print(f\"\\nNull‐space vector #{i}:\")\n",
    "    for col, coeff in zip(X.columns, v):\n",
    "        if abs(coeff) > 1e-6:\n",
    "            print(f\"  {coeff:+.3f} × {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7021f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1687, 8)\n",
      "Rank : 5\n",
      "Cond : 7.668850724557086e+35\n",
      "                    variable       VIF\n",
      "0  C(decoding_mode)[T.top_k]  3.058715\n",
      "5        decoder_temperature  2.909338\n",
      "6             sampling_param  2.006183\n",
      "1  C(decoding_mode)[T.top_p]  1.845121\n",
      "2       batch_size_numeric_c       NaN\n",
      "3            num_processes_c       NaN\n",
      "4           precision_bits_c       NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/zwzkyxhs7h7759872hzp3mz80000gn/T/ipykernel_5872/810911859.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3[col + '_c'] = df3[col] - df3[col].mean()\n",
      "/var/folders/9b/zwzkyxhs7h7759872hzp3mz80000gn/T/ipykernel_5872/810911859.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3[col + '_c'] = df3[col] - df3[col].mean()\n",
      "/var/folders/9b/zwzkyxhs7h7759872hzp3mz80000gn/T/ipykernel_5872/810911859.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3[col + '_c'] = df3[col] - df3[col].mean()\n",
      "/Users/henrybaker/miniconda3/envs/thesis/lib/python3.12/site-packages/statsmodels/regression/linear_model.py:1784: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - self.ssr/self.uncentered_tss\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import patsy\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# 1) Mean‐center your numeric predictors\n",
    "for col in ['batch_size_numeric','num_processes','precision_bits']:\n",
    "    df3[col + '_c'] = df3[col] - df3[col].mean()\n",
    "\n",
    "# 2) Build the new formula replacing raw with centered\n",
    "formula_c = (\n",
    "    \"energy_per_token_kwh ~ batch_size_numeric_c + num_processes_c + \"\n",
    "    \"precision_bits_c + decoder_temperature + sampling_param + \"\n",
    "    \"C(decoding_mode)\"\n",
    ")\n",
    "\n",
    "# 3) Reconstruct the design matrix\n",
    "y, Xc = patsy.dmatrices(formula_c, df3, return_type='dataframe')\n",
    "\n",
    "print(\"Shape:\", Xc.shape)\n",
    "print(\"Rank :\", np.linalg.matrix_rank(Xc.values))\n",
    "print(\"Cond :\", np.linalg.cond(Xc.values))\n",
    "\n",
    "# 4) VIFs (drop intercept before VIF)\n",
    "Xc_noint = Xc.drop(columns='Intercept')\n",
    "vif = (\n",
    "    pd.DataFrame({\n",
    "        'variable': Xc_noint.columns,\n",
    "        'VIF': [variance_inflation_factor(Xc_noint.values, i)\n",
    "                for i in range(Xc_noint.shape[1])]\n",
    "    })\n",
    "    .sort_values('VIF', ascending=False)\n",
    ")\n",
    "print(vif)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
