\section{Problem Statement}
\label{sec:prob_statement}

The adoption of LLMs across varied digital services has led to increased scrutiny over their energy costs, as data-centre demand from AI workloads is projected to more than quadruple by 2030 \parencite{iea_energy_2025} to account for 9.1-11.7\% of total energy demand in the US by 2030 \parencite{shehabi_united_2024, georgiou_green_2022}. Moreover, recent industry reports indicate that inference-time consumption now dominates AI energy usage. Google and Meta reported 60\% and 70\% of their respective AI-driven energy consumption was inference-related \parencite{patterson_carbon_2021, pasek_world_2023}, as was 80-90\% of Amazon Web Services’ ML cloud compute \parencite{amazon_amazon_2019}. Accurately estimating and reducing inference-time energy is vital to broader AI sustainability efforts.

Within the ML community, \textcite{strubell_energy_2019} first drew widespread academic attention to the substantial carbon costs of NLP model training, and \textcite{schwartz_green_2020} proceeded to formalise \textit{Green AI} as an efficiency-oriented alternative to the prevailing \textit{Red AI} paradigm of performance at any cost. This framing catalysed a wave of research quantifying the environmental impacts of ML \parencite{henderson_towards_2020, lacoste_quantifying_2019, garcia-martin_estimation_2019, patterson_carbon_2021}. Parallel efforts have sought to identify model-level drivers of energy consumption \parencite{cai_neuralpower_2017,luccioni_power_2024,tripp_measuring_2024}, while systems-level research has expanded upon \textcite{van_wynsberghe_sustainable_2021}'s \textit{Sustainable AI} framework, advocating for more holistic lifecycle-wide approaches to AI sustainability \parencite{gupta_chasing_2021,kaack_aligning_2022,rolnick_tackling_2023, wu_unveiling_2025}. Complementary to this, technical research has advanced energy efficiency of both training and inference through algorithmic, software, and hardware-level innovations focused on throughput and latency optimisations \parencite{dettmers_llmint8_2022, leviathan_fast_2023, aminabadi_deepspeed-_2022, chien_reducing_2023, kwon_efficient_2023, samsi_words_2023, stojkovic_towards_2024, stojkovic_dynamollm_2024, tripp_measuring_2024}.

Yet the practice of benchmarking LLM inference-time energy efficiency remains under-considered. Indeed, which empirical quantities make for practical, valid and robust measures of a model’s energy consumption, and how to measure them, remains an open question. A common simplifying assumption underlying various \textit{Green AI} efficiency metric proposals, is to take the number of FLOPs required per generated token as a proxy for inference-time energy costs \parencite{henderson_towards_2020,patterson_carbon_2021,schwartz_green_2020}. This assumption is closely aligned with parameter-counting heuristics that dominate model selection processes and continues to influence emerging AI policy discourse writ large \parencite{luccioni_power_2024}.

As measures of theoretical computational complexity, FLOP-based metrics face growing criticism for their limitation in capturing empirical energy consumption \parencite{desislavov_trends_2023,gupta_chasing_2021,fischer_energy_2023,luccioni_power_2024,tripp_measuring_2024}. Specifically, FLOP counts are analytically computed as deterministic functions of model architecture and input-output characteristics (or number of complete forward passes required), often expressed as:

$$\text{FLOPs} = f(\text{number of parameters, input length, output length})$$

FLOPs quantify the number of arithmetic operations required to generate a given output, but they do not capture the energy efficiency with which those operations are executed. While correlated, FLOPs and inference-time energy consumption are conceptually distinct. The latter is a mediated empirical measure, shaped both by upstream (of FLOPs) development-phase choices, which define the static computation graph - such as model architecture and parameterisation - and downstream deployment-phase choices, which govern how efficiently that graph is executed in practice. Emerging evidence indicates that implementation-level factors can induce substantial variation in a deployment's energy consumption, even when FLOPs counts remain constant. Two systems with the same theoretical compute requirement may exhibit markedly different energy profiles. This makes raw FLOPs an incomplete and potentially misleading proxy for real-world energy costs. 

Notably, recent initiatives such as Hugging Face’s \textit{AI Energy Score} Leaderboard \parencite{noauthor_ai_2025} opt for direct empirical energy measures. Nevertheless, despite growing recognition of the limitations of theoretical proxies, FLOP-counts (and less frequently, multiply-accumulation operations (MAC) counts) remain widely used in both academic and industry contexts for estimating energy costs, due to their ease of computation from static model specifications. However, this research demonstrates that reporting FLOPs-per-token in isolation fails to capture the substantial range of empirical variation observable in inference-time energy consumption across different implementation configurations, nor provides information as to the underlying mechanisms driving this variance.

Moreover, FLOP-counting narrows policy attention to immutable model attributes, conceptually restricting the scope of intervention to the moment of model selection. This framing overlooks a wide array of downstream system-level implementation decisions and tradeoffs that shape the energy efficiency of real-world deployments \parencite{luccioni_power_2024,gupta_chasing_2021}. From a benchmarking perspective, the neglect of implementation-level variation translates through to a lack of standardised test-time controls. This gap creates opportunities for motivated actors to present artificially efficient performance metrics by testing under unrealistic configurations, misaligned with production constraints. Ultimately, these dynamics leave, consumers and policymakers with little insight into the true energy cost of querying an LLM, or the trade-offs being made on their behalf \parencite{luccioni_power_2024,gupta_chasing_2021}.

In this research, a subset of these implementation decisions are treated as tunable parameters to evaluate how deployment choices influence inference efficiency. The comprehensive grid search identifies substantial variability in energy efficiency - up to 516.5-fold for the LLaMA-3.2-1B model and 293.5-fold for the 3B model - with corresponding coefficients of variation of 127.7\% and 123.5\%, respectively (see Table~\ref{tab:energy_stats}). While the explored parameter space includes many impractical and unrealistic configurations - meaning these extreme figures should not be interpreted as directly achievable gains in real-world production - the results underscore the importance of standardised benchmarking methods representative of real-world operational contexts.

\newpage