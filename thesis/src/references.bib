
@article{pasek_world_2023,
	title = {The world wide web of carbon: {Toward} a relational footprinting of information and communications technology's climate impacts},
	volume = {10},
	issn = {2053-9517, 2053-9517},
	shorttitle = {The world wide web of carbon},
	url = {https://journals.sagepub.com/doi/10.1177/20539517231158994},
	doi = {10.1177/20539517231158994},
	abstract = {The climate impacts of the information and communications technology sector—and Big Data especially—is a topic of growing public and industry concern, though attempts to quantify its carbon footprint have produced contradictory results. Some studies argue that information and communications technology’s global carbon footprint is set to rise dramatically in the coming years, requiring urgent regulation and sectoral degrowth. Others argue that information and communications technology’s growth is largely decoupled from its carbon emissions, and so provides valuable climate solutions and a model for other industries. This article assesses these debates, arguing that, due to data frictions and incommensurate study designs, the question is likely to remain irresolvable at the global scale. We present six methodological factors that drive this impasse: fraught access to industry data, bottom-up vs. top-down assessments, system boundaries, geographic averaging, functional units, and energy efﬁciencies. In response, we propose an alternative approach that reframes the question in spatial and situated terms: A relational footprinting that demarcates particular relationships between elements—geographic, technical, and social—within broader information and communications technology infrastructures. Illustrating this model with one of the global Internet’s most overlooked components—subsea telecommunication cables—we propose that information and communications technology futures would be best charted not only in terms of quantiﬁed total energy use, but in specifying the geographical and technical parts of the network that are the least carbon-intensive, and which can therefore provide opportunities for both carbon reductions and a renewed infrastructural politics. In parallel to the politics of (de)growth, we must also consider different network forms.},
	language = {en},
	number = {1},
	urldate = {2024-12-19},
	journal = {Big Data \& Society},
	author = {Pasek, Anne and Vaughan, Hunter and Starosielski, Nicole},
	month = jan,
	year = {2023},
	pages = {20539517231158994},
	file = {Pasek et al. - 2023 - The world wide web of carbon Toward a relational .pdf:/Users/henrybaker/Zotero/storage/LV2KSIFC/Pasek et al. - 2023 - The world wide web of carbon Toward a relational .pdf:application/pdf},
}

@misc{strubell_energy_2019,
	title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},
	url = {http://arxiv.org/abs/1906.02243},
	doi = {10.48550/arXiv.1906.02243},
	abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both ﬁnancially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate ﬁnancial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these ﬁndings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
	language = {en},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = jun,
	year = {2019},
	note = {arXiv:1906.02243 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning.pdf:/Users/henrybaker/Zotero/storage/CC9AVT68/Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning.pdf:application/pdf},
}

@misc{luccioni_estimating_2022,
	title = {Estimating the {Carbon} {Footprint} of {BLOOM}, a {176B} {Parameter} {Language} {Model}},
	url = {http://arxiv.org/abs/2211.02001},
	doi = {10.48550/arXiv.2211.02001},
	abstract = {Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires signiﬁcant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM’s ﬁnal training emitted approximately 24.7 tonnes of CO2eq if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difﬁculty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.},
	language = {en},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02001 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Luccioni et al. - 2022 - Estimating the Carbon Footprint of BLOOM, a 176B P.pdf:/Users/henrybaker/Zotero/storage/V52D7B39/Luccioni et al. - 2022 - Estimating the Carbon Footprint of BLOOM, a 176B P.pdf:application/pdf},
}

@article{kaack_aligning_2022,
	title = {Aligning artificial intelligence with climate change mitigation},
	volume = {12},
	issn = {1758-678X, 1758-6798},
	url = {https://www.nature.com/articles/s41558-022-01377-7},
	doi = {10.1038/s41558-022-01377-7},
	language = {en},
	number = {6},
	urldate = {2024-12-19},
	journal = {Nature Climate Change},
	author = {Kaack, Lynn H. and Donti, Priya L. and Strubell, Emma and Kamiya, George and Creutzig, Felix and Rolnick, David},
	month = jun,
	year = {2022},
	pages = {518--527},
	file = {Kaack et al. - 2022 - Aligning artificial intelligence with climate chan.pdf:/Users/henrybaker/Zotero/storage/V7L6XNWK/Kaack et al. - 2022 - Aligning artificial intelligence with climate chan.pdf:application/pdf},
}

@article{milojevic-dupont_learning_2020,
	title = {Learning from urban form to predict building heights},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0242010},
	doi = {10.1371/journal.pone.0242010},
	abstract = {Understanding cities as complex systems, sustainable urban planning depends on reliable high-resolution data, for example of the building stock to upscale region-wide retrofit policies. For some cities and regions, these data exist in detailed 3D models based on realworld measurements. However, they are still expensive to build and maintain, a significant challenge, especially for small and medium-sized cities that are home to the majority of the European population. New methods are needed to estimate relevant building stock characteristics reliably and cost-effectively. Here, we present a machine learning based method for predicting building heights, which is based only on open-access geospatial data on urban form, such as building footprints and street networks. The method allows to predict building heights for regions where no dedicated 3D models exist currently. We train our model using building data from four European countries (France, Italy, the Netherlands, and Germany) and find that the morphology of the urban fabric surrounding a given building is highly predictive of the height of the building. A test on the German state of Brandenburg shows that our model predicts building heights with an average error well below the typical floor height (about 2.5 m), without having access to training data from Germany. Furthermore, we show that even a small amount of local height data obtained by citizens substantially improves the prediction accuracy. Our results illustrate the possibility of predicting missing data on urban infrastructure; they also underline the value of open government data and volunteered geographic information for scientific applications, such as contextual but scalable strategies to mitigate climate change.},
	language = {en},
	number = {12},
	urldate = {2024-12-19},
	journal = {PLOS ONE},
	author = {Milojevic-Dupont, Nikola and Hans, Nicolai and Kaack, Lynn H. and Zumwald, Marius and Andrieux, François and De Barros Soares, Daniel and Lohrey, Steffen and Pichler, Peter-Paul and Creutzig, Felix},
	editor = {Biljecki, Filip},
	month = dec,
	year = {2020},
	pages = {e0242010},
	file = {Milojevic-Dupont et al. - 2020 - Learning from urban form to predict building heigh.pdf:/Users/henrybaker/Zotero/storage/VXRHU5DZ/Milojevic-Dupont et al. - 2020 - Learning from urban form to predict building heigh.pdf:application/pdf},
}

@article{sewerin_towards_2023,
	title = {Towards understanding policy design through text-as-data approaches: {The} policy design annotations ({POLIANNA}) dataset},
	volume = {10},
	issn = {2052-4463},
	shorttitle = {Towards understanding policy design through text-as-data approaches},
	url = {https://www.nature.com/articles/s41597-023-02801-z},
	doi = {10.1038/s41597-023-02801-z},
	abstract = {Abstract
            Despite the importance of ambitious policy action for addressing climate change, large and systematic assessments of public policies and their design are lacking as analysing text manually is labour-intensive and costly. POLIANNA is a dataset of policy texts from the European Union (EU) that are annotated based on theoretical concepts of policy design, which can be used to develop supervised machine learning approaches for scaling policy analysis. The dataset consists of 20,577 annotated spans, drawn from 18 EU climate change mitigation and renewable energy policies. We developed a novel coding scheme translating existing taxonomies of policy design elements to a method for annotating text spans that consist of one or several words. Here, we provide the coding scheme, a description of the annotated corpus, and an analysis of inter-annotator agreement, and discuss potential applications. As understanding policy texts is still difficult for current text-processing algorithms, we envision this database to be used for building tools that help with manual coding of policy texts by automatically proposing paragraphs containing relevant information.},
	language = {en},
	number = {1},
	urldate = {2024-12-19},
	journal = {Scientific Data},
	author = {Sewerin, Sebastian and Kaack, Lynn H. and Küttel, Joel and Sigurdsson, Fride and Martikainen, Onerva and Esshaki, Alisha and Hafner, Fabian},
	month = dec,
	year = {2023},
	pages = {896},
	file = {Sewerin et al. - 2023 - Towards understanding policy design through text-a.pdf:/Users/henrybaker/Zotero/storage/YDKC6Z9P/Sewerin et al. - 2023 - Towards understanding policy design through text-a.pdf:application/pdf},
}

@article{rolnick_tackling_2023,
	title = {Tackling {Climate} {Change} with {Machine} {Learning}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3485128},
	doi = {10.1145/3485128},
	abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.},
	language = {en},
	number = {2},
	urldate = {2024-12-19},
	journal = {ACM Computing Surveys},
	author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra Sasha and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla P. and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
	month = feb,
	year = {2023},
	pages = {1--96},
	file = {Rolnick et al. - 2023 - Tackling Climate Change with Machine Learning.pdf:/Users/henrybaker/Zotero/storage/AGVX7RFY/Rolnick et al. - 2023 - Tackling Climate Change with Machine Learning.pdf:application/pdf},
}

@article{guan_wattscope_2023,
	title = {{WattScope}: {Non}-intrusive application-level power disaggregation in datacenters},
	volume = {162},
	issn = {01665316},
	shorttitle = {{WattScope}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166531623000391},
	doi = {10.1016/j.peva.2023.102369},
	abstract = {Datacenter capacity is growing exponentially to satisfy the increasing demand for many emerging computationally-intensive applications, such as deep learning. This trend has led to concerns over datacenters’ increasing energy consumption and carbon footprint. The most basic prerequisite for optimizing a datacenter’s energy- and carbon-efficiency is accurately monitoring and attributing energy consumption to specific users and applications. Since datacenter servers tend to be multi-tenant, i.e., they host many applications, server- and rack-level power monitoring alone does not provide insight into the energy usage and carbon emissions of their resident applications. At the same time, current application-level energy monitoring and attribution techniques are intrusive: they require privileged access to servers and necessitate coordinated support in hardware and software, neither of which is always possible in cloud environments. To address the problem, we design WattScope, a system for non-intrusively estimating the power consumption of individual applications using external measurements of a server’s aggregate power usage and without requiring direct access to the server’s operating system or applications. Our key insight is that, based on an analysis of production traces, the power characteristics of datacenter workloads, e.g., low variability, low magnitude, and high periodicity, are highly amenable to disaggregation of a server’s total power consumption into application-specific values. WattScope adapts and extends a machine learning-based technique for disaggregating building power and applies it to server- and rack-level power meter measurements that are already available in data centers. We evaluate WattScope’s accuracy on a production workload and show that it yields high accuracy, e.g., often {\textless}∼10\% normalized mean absolute error, and is thus a potentially useful tool for datacenters in externally monitoring application-level power usage.},
	language = {en},
	urldate = {2024-12-19},
	journal = {Performance Evaluation},
	author = {Guan, Xiaoding and Bashir, Noman and Irwin, David and Shenoy, Prashant},
	month = nov,
	year = {2023},
	pages = {102369},
	file = {Guan et al. - 2023 - WattScope Non-intrusive application-level power d.pdf:/Users/henrybaker/Zotero/storage/JT6Z9ZY5/Guan et al. - 2023 - WattScope Non-intrusive application-level power d.pdf:application/pdf},
}

@article{budennyy_eco2ai_2022,
	title = {{eco2AI}: {Carbon} {Emissions} {Tracking} of {Machine} {Learning} {Models} as the {First} {Step} {Towards} {Sustainable} {AI}},
	volume = {106},
	issn = {1064-5624, 1531-8362},
	shorttitle = {{eco2AI}},
	url = {https://link.springer.com/10.1134/S1064562422060230},
	doi = {10.1134/S1064562422060230},
	abstract = {The size and complexity of deep neural networks used in AI applications continue to grow exponentially, significantly increasing energy consumption for training and inference by these models. We introduce an open-source package eco2AI to help data scientists and researchers to track the energy consumption and equivalent CO2 emissions of their models in a straightforward way. In eco2AI we focus on accurate tracking of energy consumption and regional CO2 emissions accounting. We encourage the research for community to search for new optimal Artificial Intelligence (AI) architectures with lower computational cost. The motivation also comes from the concept of AI-based greenhouse gases sequestrating cycle with both Sustainable AI and Green AI pathways. The code and documentation are hosted on Github under the Apache 2.0 license https://github.com/sb-ai-lab/Eco2AI.},
	language = {en},
	number = {S1},
	urldate = {2024-12-20},
	journal = {Doklady Mathematics},
	author = {Budennyy, S. A. and Lazarev, V. D. and Zakharenko, N. N. and Korovin, A. N. and Plosskaya, O. A. and Dimitrov, D. V. and Akhripkin, V. S. and Pavlov, I. V. and Oseledets, I. V. and Barsola, I. S. and Egorov, I. V. and Kosterina, A. A. and Zhukov, L. E.},
	month = dec,
	year = {2022},
	pages = {S118--S128},
	file = {Budennyy et al. - 2022 - eco2AI Carbon Emissions Tracking of Machine Learn.pdf:/Users/henrybaker/Zotero/storage/D7BM4E7V/Budennyy et al. - 2022 - eco2AI Carbon Emissions Tracking of Machine Learn.pdf:application/pdf},
}

@misc{anthony_carbontracker_2020,
	title = {Carbontracker: {Tracking} and {Predicting} the {Carbon} {Footprint} of {Training} {Deep} {Learning} {Models}},
	shorttitle = {Carbontracker},
	url = {http://arxiv.org/abs/2007.03051},
	doi = {10.48550/arXiv.2007.03051},
	abstract = {Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.},
	language = {en},
	urldate = {2024-12-20},
	publisher = {arXiv},
	author = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},
	month = jul,
	year = {2020},
	note = {arXiv:2007.03051 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {Anthony et al. - 2020 - Carbontracker Tracking and Predicting the Carbon .pdf:/Users/henrybaker/Zotero/storage/ZT29LCY6/Anthony et al. - 2020 - Carbontracker Tracking and Predicting the Carbon .pdf:application/pdf},
}

@article{tiutiulnikov_eco4cast_2023,
	title = {eco4cast: {Bridging} {Predictive} {Scheduling} and {Cloud} {Computing} for {Reduction} of {Carbon} {Emissions} for {ML} {Models} {Training}},
	volume = {108},
	issn = {1064-5624, 1531-8362},
	shorttitle = {eco4cast},
	url = {https://link.springer.com/10.1134/S1064562423701223},
	doi = {10.1134/S1064562423701223},
	abstract = {We introduce eco4cast,1 an open-source package aimed to reduce carbon footprint of machine learning models via predictive cloud computing scheduling. The package is integrated with machine learning models and employs an advanced temporal convolution neural network to forecast daily carbon dioxide emissions stemming from electricity generation.The model attains remarkable predictive accuracy by accounting for weather conditions, acknowledged for their robust correlation with carbon energy intensity. The hallmark of eco4cast lies in its capability to identify periods of temporal minimal carbon intensity. This enables the package to manage cloud computing tasks only during these periods, significantly reducing the ecological impact. Our contribution represents a compelling fusion of sustainability and computational efficiency. The code and documentation of the package are hosted on GitHub under the Apache 2.0 license.},
	language = {en},
	number = {S2},
	urldate = {2024-12-20},
	journal = {Doklady Mathematics},
	author = {Tiutiulnikov, M. and Lazarev, V. and Korovin, A. and Zakharenko, N. and Doroshchenko, I. and Budennyy, S.},
	month = dec,
	year = {2023},
	pages = {S443--S455},
	file = {Tiutiulnikov et al. - 2023 - eco4cast Bridging Predictive Scheduling and Cloud.pdf:/Users/henrybaker/Zotero/storage/G35E4B9K/Tiutiulnikov et al. - 2023 - eco4cast Bridging Predictive Scheduling and Cloud.pdf:application/pdf},
}

@inproceedings{castano_exploring_2023,
	title = {Exploring the {Carbon} {Footprint} of {Hugging} {Face}'s {ML} {Models}: {A} {Repository} {Mining} {Study}},
	shorttitle = {Exploring the {Carbon} {Footprint} of {Hugging} {Face}'s {ML} {Models}},
	url = {http://arxiv.org/abs/2305.11164},
	doi = {10.1109/ESEM56168.2023.10304801},
	abstract = {Background: The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. Aims: This paper analyzes the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face. Hugging Face is the most popular repository for pretrained ML models. We aim to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. Method: We conduct the first repository mining study on the Hugging Face Hub API on carbon emissions and answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? Results: Key findings from the study include a stalled proportion of carbon emissions-reporting models, a slight decrease in reported carbon footprint on Hugging Face over the past 2 years, and a continued dominance of NLP as the main application domain reporting emissions. The study also uncovers correlations between carbon emissions and various attributes, such as model size, dataset size, ML application domains and performance metrics. Conclusions: The results emphasize the need for software measurements to improve energy reporting practices and the promotion of carbonefficient model development within the Hugging Face community. To address this issue, we propose two classifications: one for categorizing models based on their carbon emission reporting practices and another for their carbon efficiency. With these classification proposals, we aim to encourage transparency and sustainable model development within the ML community.},
	language = {en},
	urldate = {2024-12-20},
	booktitle = {2023 {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Castaño, Joel and Martínez-Fernández, Silverio and Franch, Xavier and Bogner, Justus},
	month = oct,
	year = {2023},
	note = {arXiv:2305.11164 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	pages = {1--12},
	file = {Castaño et al. - 2023 - Exploring the Carbon Footprint of Hugging Face's M.pdf:/Users/henrybaker/Zotero/storage/EXS4D2AX/Castaño et al. - 2023 - Exploring the Carbon Footprint of Hugging Face's M.pdf:application/pdf},
}

@incollection{selvan_carbon_2022,
	title = {Carbon {Footprint} of {Selecting} and {Training} {Deep} {Learning} {Models} for {Medical} {Image} {Analysis}},
	volume = {13435},
	url = {http://arxiv.org/abs/2203.02202},
	abstract = {The increasing energy consumption and carbon footprint of deep learning (DL) due to growing compute requirements has become a cause of concern. In this work, we focus on the carbon footprint of developing DL models for medical image analysis (MIA), where volumetric images of high spatial resolution are handled. In this study, we present and compare the features of four tools from literature to quantify the carbon footprint of DL. Using one of these tools we estimate the carbon footprint of medical image segmentation pipelines. We choose nnU-net as the proxy for a medical image segmentation pipeline and experiment on three common datasets. With our work we hope to inform on the increasing energy costs incurred by MIA. We discuss simple strategies to cut-down the environmental impact that can make model selection and training processes more efficient.},
	language = {en},
	urldate = {2024-12-20},
	author = {Selvan, Raghavendra and Bhagwat, Nikhil and Anthony, Lasse F. Wolff and Kanding, Benjamin and Dam, Erik B.},
	year = {2022},
	doi = {10.1007/978-3-031-16443-9_49},
	note = {arXiv:2203.02202 [eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {506--516},
	file = {Selvan et al. - 2022 - Carbon Footprint of Selecting and Training Deep Le.pdf:/Users/henrybaker/Zotero/storage/UAJRAF5N/Selvan et al. - 2022 - Carbon Footprint of Selecting and Training Deep Le.pdf:application/pdf},
}

@article{savazzi_energy_2023,
	title = {An {Energy} and {Carbon} {Footprint} {Analysis} of {Distributed} and {Federated} {Learning}},
	volume = {7},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2473-2400},
	url = {https://ieeexplore.ieee.org/document/9807354/},
	doi = {10.1109/TGCN.2022.3186439},
	abstract = {Classical and centralized Artiﬁcial Intelligence (AI) methods require moving data from producers (sensors, machines) to energy hungry data centers, raising environmental concerns due to computational and communication resource demands, while violating privacy. Emerging alternatives to mitigate such high energy costs propose to efﬁciently distribute, or federate, the learning tasks across devices, which are typically low-power. This paper proposes a novel framework for the analysis of energy and carbon footprints in distributed and federated learning (FL). The proposed framework quantiﬁes both the energy footprints and the carbon equivalent emissions for vanilla FL methods and consensus-based fully decentralized approaches. We discuss optimal bounds and operational points that support green FL designs and underpin their sustainability assessment. Two case studies from emerging 5G industry verticals are analyzed: these quantify the environmental footprints of continual and reinforcement learning setups, where the training process is repeated periodically for continuous improvements. For all cases, sustainability of distributed learning relies on the fulﬁllment of speciﬁc requirements on communication efﬁciency and learner population size. Energy and test accuracy should be also traded off considering the model and the data footprints for the targeted industrial applications.},
	language = {en},
	number = {1},
	urldate = {2024-12-20},
	journal = {IEEE Transactions on Green Communications and Networking},
	author = {Savazzi, Stefano and Rampa, Vittorio and Kianoush, Sanaz and Bennis, Mehdi},
	month = mar,
	year = {2023},
	pages = {248--264},
	file = {Savazzi et al. - 2023 - An Energy and Carbon Footprint Analysis of Distrib.pdf:/Users/henrybaker/Zotero/storage/MEH9AJJL/Savazzi et al. - 2023 - An Energy and Carbon Footprint Analysis of Distrib.pdf:application/pdf},
}

@article{axberg_deriving_2022,
	title = {Deriving an {Natural} {Language} {Processing} inference {Cost} {Model} with {Greenhouse} {Gas} {Accounting}: {Towards} a sustainable usage of {Machine} {Learning}},
	abstract = {The interest in using State-Of-The-Art (SOTA) Pre-Trained Language Models (PLMs) in product development is growing. The fact that developers can use PLMs has changed the way to build reliable models, and it is the goto method for many companies and organizations. Selecting the Natural Language Processing (NLP) model with the highest accuracy is the usual way of deciding which PLM to use. However, with growing concerns about negative climate changes, we need new ways of making decisions that consider the impact on our future needs. The best solution with the highest accuracy might not be the best choice when other parameters matter, such as sustainable development.},
	language = {en},
	author = {Axberg, Tom},
	year = {2022},
	file = {Axberg - Deriving an Natural Language Processing inference .pdf:/Users/henrybaker/Zotero/storage/8HXJ3JAE/Axberg - Deriving an Natural Language Processing inference .pdf:application/pdf},
}

@article{patterson_carbon_2021,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models—T5, Meena, GShard, Switch Transformer, and GPT-3—and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): ● Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. ● Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. ● Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	language = {en},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	year = {2021},
	file = {Patterson et al. - Carbon Emissions and Large Neural Network Training.pdf:/Users/henrybaker/Zotero/storage/B2FG3KAF/Patterson et al. - Carbon Emissions and Large Neural Network Training.pdf:application/pdf},
}

@article{paccou_artificial_2024,
	title = {Artificial {Intelligence} and {Electricity}},
	language = {en},
	author = {Paccou, Rémi and Wijnhoven, Fons},
	month = dec,
	year = {2024},
	file = {Paccou and Wijnhoven - Artificial Intelligence and Electricity.pdf:/Users/henrybaker/Zotero/storage/G2RDVRJW/Paccou and Wijnhoven - Artificial Intelligence and Electricity.pdf:application/pdf},
}

@misc{stojkovic_dynamollm_2024,
	title = {{DynamoLLM}: {Designing} {LLM} {Inference} {Clusters} for {Performance} and {Energy} {Efficiency}},
	shorttitle = {{DynamoLLM}},
	url = {http://arxiv.org/abs/2408.00741},
	doi = {10.48550/arXiv.2408.00741},
	abstract = {The rapid evolution and widespread adoption of generative large language models (LLMs) have made them a pivotal workload in various applications. Today, LLM inference clusters receive a large number of queries with strict Service Level Objectives (SLOs). To achieve the desired performance, these models execute on power-hungry GPUs causing the inference clusters to consume large amount of energy and, consequently, result in excessive carbon emissions. Fortunately, we find that there is a great opportunity to exploit the heterogeneity in inference compute properties and fluctuations in inference workloads, to significantly improve energy-efficiency. However, such a diverse and dynamic environment creates a large search-space where different system configurations (e.g., number of instances, model parallelism, and GPU frequency) translate into different energy-performance trade-offs. To address these challenges, we propose DynamoLLM, the first energy-management framework for LLM inference environments. DynamoLLM automatically and dynamically reconfigures the inference cluster to optimize for energy and cost of LLM serving under the service’s performance SLOs. We show that at a service-level, DynamoLLM conserves 53\% energy and 38\% operational carbon emissions, and reduces 61\% cost to the customer, while meeting the latency SLOs.},
	language = {en},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Stojkovic, Jovan and Zhang, Chaojie and Goiri, Íñigo and Torrellas, Josep and Choukse, Esha},
	month = aug,
	year = {2024},
	note = {arXiv:2408.00741 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {Stojkovic et al. - 2024 - DynamoLLM Designing LLM Inference Clusters for Pe.pdf:/Users/henrybaker/Zotero/storage/6AGRCFQJ/Stojkovic et al. - 2024 - DynamoLLM Designing LLM Inference Clusters for Pe.pdf:application/pdf},
}

@article{weng_mlaas_2022,
	title = {{MLaaS} in the {Wild}: {Workload} {Analysis} and {Scheduling} in {Large}-{Scale} {Heterogeneous} {GPU} {Clusters}},
	abstract = {With the sustained technological advances in machine learning (ML) and the availability of massive datasets recently, tech companies are deploying large ML-as-a-Service (MLaaS) clouds, often with heterogeneous GPUs, to provision a host of ML applications. However, running diverse ML workloads in heterogeneous GPU clusters raises a number of challenges. In this paper, we present a characterization study of a two-month workload trace collected from a production MLaaS cluster with over 6,000 GPUs in Alibaba. We explain the challenges posed to cluster scheduling, including the low GPU utilization, the long queueing delays, the presence of hard-to-schedule tasks demanding high-end GPUs with picky scheduling requirements, the imbalance load across heterogeneous machines, and the potential bottleneck on CPUs. We describe our current solutions and call for further investigations into the challenges that remain open to address. We have released the trace for public access, which is the most comprehensive in terms of the workloads and cluster scale.},
	language = {en},
	author = {Weng, Qizhen and Xiao, Wencong and Yu, Yinghao and Wang, Wei and Wang, Cheng and He, Jian and Li, Yong and Zhang, Liping and Lin, Wei and Ding, Yu},
	year = {2022},
	file = {Weng et al. - MLaaS in the Wild Workload Analysis and Schedulin.pdf:/Users/henrybaker/Zotero/storage/8FEKEI5J/Weng et al. - MLaaS in the Wild Workload Analysis and Schedulin.pdf:application/pdf},
}

@article{stocker_ict_2024,
	title = {{ICT} {Sustainability} {Reporting} {Strategies} of {Large} {Tech} {Companies}: {Changes} in {Format}, {Scope}, and {Content}},
	issn = {1556-5068},
	shorttitle = {{ICT} {Sustainability} {Reporting} {Strategies} of {Large} {Tech} {Companies}},
	url = {https://www.ssrn.com/abstract=4927128},
	doi = {10.2139/ssrn.4927128},
	abstract = {The environmental impact of large tech companies has come under intense scrutiny, particularly as the COVID-19 pandemic and the frenzy about Generative AI have driven increased demand for ICT infrastructures and services. Unsurprisingly, calls for more transparency and the disclosure of sustainability-related metrics by these companies have grown over time. This paper explores the sustainability reporting strategies of large tech firms, with a focus on decarbonization and data centers. We examine critical areas such as greenhouse gas emissions, renewable energy usage, and energy efficiency through a longitudinal comparative analysis of sustainability reporting activities from 2003 to 2023. Our sample includes nine leading tech companies from the US (Akamai, Amazon, Apple, Facebook/Meta, Google/Alphabet, Microsoft) and China (Alibaba, Baidu, Tencent). The analysis highlights a convergence among these companies towards publishing dedicated sustainability reports, providing more comprehensive and nuanced insights into supply chains and data centers, and emphasizing carbon neutrality and 100\% renewable energy usage. A notable trend is the shift in focus from energy efficiency to renewable energy supply and from operational emissions to value chain emissions. Based on a critical discussion, we offer valuable insights for different stakeholders.},
	language = {en},
	urldate = {2025-01-11},
	journal = {SSRN Electronic Journal},
	author = {Stocker, Volker and Mariotte, Niklas and Ullrich, André and Rejeski, David},
	year = {2024},
	file = {Stocker et al. - 2024 - ICT Sustainability Reporting Strategies of Large T.pdf:/Users/henrybaker/Zotero/storage/X9NJQLD9/Stocker et al. - 2024 - ICT Sustainability Reporting Strategies of Large T.pdf:application/pdf},
}

@article{li_chinas_2023,
	title = {China's green data center development:{Policies} and carbon reduction technology path},
	volume = {231},
	issn = {00139351},
	shorttitle = {China's green data center development},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0013935123010526},
	doi = {10.1016/j.envres.2023.116248},
	language = {en},
	urldate = {2025-01-11},
	journal = {Environmental Research},
	author = {Li, Guozhu and Sun, Zixuan and Wang, Qingqin and Wang, Shuai and Huang, Kailiang and Zhao, Naini and Di, Yanqiang and Zhao, Xudong and Zhu, Zishang},
	month = aug,
	year = {2023},
	pages = {116248},
	file = {Li et al. - 2023 - China's green data center developmentPolicies and.pdf:/Users/henrybaker/Zotero/storage/FVICT2FP/Li et al. - 2023 - China's green data center developmentPolicies and.pdf:application/pdf},
}

@article{ni_co2_2024,
	title = {{CO2} emission-mitigation pathways for {China}'s data centers},
	volume = {202},
	issn = {09213449},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921344923005177},
	doi = {10.1016/j.resconrec.2023.107383},
	abstract = {Increased emissions related to China’s burgeoning digital economy pose significant challenges. Using a Kaya–LMDI model, this study investigates the driving factors of data-center CO2 emissions in China from 2017 to 2021, highlighting the roles of computing scale, energy intensity, power usage effectiveness, and emission in­ tensity. We find a marked increase in emissions across various Chinese provinces, largely driven by computing scale. While projections suggest that data-center emissions could reach 430 million tons by 2050 (three times greater than 2021 levels), such emissions could potentially be reduced to 11–29 million tons under the “net-zero emissions” scenario. Highlighting the need to mitigate data-center emission intensity, our findings underscore the recalibrations in operational methods, technology, and energy sourcing needed to expedite the transition to net-zero emissions.},
	language = {en},
	urldate = {2025-01-11},
	journal = {Resources, Conservation and Recycling},
	author = {Ni, Wenli and Hu, Xiurong and Du, Hongyang and Kang, Yulin and Ju, Yi and Wang, Qunwei},
	month = mar,
	year = {2024},
	pages = {107383},
	file = {Ni et al. - 2024 - CO2 emission-mitigation pathways for China's data .pdf:/Users/henrybaker/Zotero/storage/AK3J54ZP/Ni et al. - 2024 - CO2 emission-mitigation pathways for China's data .pdf:application/pdf},
}

@inproceedings{dodge_measuring_2022,
	address = {Seoul Republic of Korea},
	title = {Measuring the {Carbon} {Intensity} of {AI} in {Cloud} {Instances}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533234},
	doi = {10.1145/3531146.3533234},
	abstract = {The advent of cloud computing has provided people around the world with unprecedented access to computational power and enabled rapid growth in technologies such as machine learning, the computational demands of which incur a high energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable access to measurements of this information, which precludes development of actionable tactics. We argue that cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions.},
	language = {en},
	urldate = {2025-01-21},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Dodge, Jesse and Prewitt, Taylor and Tachet Des Combes, Remi and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A. and DeCario, Nicole and Buchanan, Will},
	month = jun,
	year = {2022},
	pages = {1877--1894},
	file = {Dodge et al. - 2022 - Measuring the Carbon Intensity of AI in Cloud Inst.pdf:/Users/henrybaker/Zotero/storage/XCYN3BMC/Dodge et al. - 2022 - Measuring the Carbon Intensity of AI in Cloud Inst.pdf:application/pdf},
}

@article{de_vries_growing_2023,
	title = {The growing energy footprint of artificial intelligence},
	volume = {7},
	issn = {25424351},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2542435123003653},
	doi = {10.1016/j.joule.2023.09.004},
	language = {en},
	number = {10},
	urldate = {2025-01-21},
	journal = {Joule},
	author = {De Vries, Alex},
	month = oct,
	year = {2023},
	pages = {2191--2194},
	file = {De Vries - 2023 - The growing energy footprint of artificial intelli.pdf:/Users/henrybaker/Zotero/storage/CXBCBFDM/De Vries - 2023 - The growing energy footprint of artificial intelli.pdf:application/pdf},
}

@misc{ji_advancing_2024,
	title = {Advancing {Environmental} {Sustainability} in {Data} {Centers} by {Proposing} {Carbon} {Depreciation} {Models}},
	url = {http://arxiv.org/abs/2403.04976},
	doi = {10.48550/arXiv.2403.04976},
	abstract = {The rising demand for on-demand, highperformance computing has led to the growth of data centers, which in turn presents both challenges and opportunities for addressing their environmental impact. Traditionally, sustainability efforts in data centers have focused on reducing energy consumption. However, with advancements in energy efficiency and the integration of renewable energy, the role of embodied carbon has become increasingly significant, necessitating a shift in data center provisioning strategies. This paper proposes the use of carbon depreciation models to encourage longer hardware lifecycles in data centers. These models allocate a higher share of embodied carbon to newly provisioned servers, thereby incentivizing the reduction of new server acquisitions for jobs with stringent quality-of-service (QoS) requirements and promoting the extended use of existing servers with largely recovered embodied carbon. Additionally, we argue that both embodied and operational carbon from server idle time should be considered and recovered during active job processing, which supports high utilization rates. Our analysis demonstrates that traditional carbon accounting methods, which favor new hardware under QoS constraints, are counterproductive to sustainability, as they undervalue the carbon impact of older equipment by pricing jobs 25\% cheaper on new hardware. Our approach advocates for improved sustainability through our depreciation model, which ensures that jobs on new machines account for more than twice the carbon emissions compared to older machines.},
	language = {en},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Ji, Shixin and Yang, Zhuoping and Jones, Alex K. and Zhou, Peipei},
	month = aug,
	year = {2024},
	note = {arXiv:2403.04976 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Ji et al. - 2024 - Advancing Environmental Sustainability in Data Cen.pdf:/Users/henrybaker/Zotero/storage/8W5ZXB8Y/Ji et al. - 2024 - Advancing Environmental Sustainability in Data Cen.pdf:application/pdf},
}

@misc{lacoste_quantifying_2019,
	title = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1910.09700},
	doi = {10.48550/arXiv.1910.09700},
	abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
	language = {en},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
	month = nov,
	year = {2019},
	note = {arXiv:1910.09700 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learni.pdf:/Users/henrybaker/Zotero/storage/SJXB74MA/Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learni.pdf:application/pdf},
}

@article{hogade_game-theoretic_2024,
	title = {Game-{Theoretic} {Deep} {Reinforcement} {Learning} to {Minimize} {Carbon} {Emissions} and {Energy} {Costs} for {AI} {Inference} {Workloads} in {Geo}-{Distributed} {Data} {Centers}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2377-3782, 2377-3790},
	url = {https://ieeexplore.ieee.org/document/10811780/},
	doi = {10.1109/TSUSC.2024.3520969},
	abstract = {Data centers are increasingly using more energy due to the rise in Artificial Intelligence (AI) workloads, which negatively impacts the environment and raises operational costs. Reducing operating expenses and carbon emissions while maintaining performance in data centers is a challenging problem. This work introduces a unique approach combining Game Theory (GT) and Deep Reinforcement Learning (DRL) for optimizing the distribution of AI inference workloads in geo-distributed data centers to reduce carbon emissions and cloud operating (energy + data transfer) costs. The proposed technique integrates the principles of non-cooperative Game Theory into a DRL framework, enabling data centers to make intelligent decisions regarding workload allocation while considering the heterogeneity of hardware resources, the dynamic nature of electricity prices, inter-data center data transfer costs, and carbon footprints. We conducted extensive experiments comparing our game-theoretic DRL (GT-DRL) approach with current DRL-based and other optimization techniques. The results demonstrate that our strategy outperforms the state-of-the-art in reducing carbon emissions and minimizing cloud operating costs without compromising computational performance. This work has significant implications for achieving sustainability and cost-efficiency in data centers handling AI inference workloads across diverse geographic locations.},
	language = {en},
	urldate = {2025-01-21},
	journal = {IEEE Transactions on Sustainable Computing},
	author = {Hogade, Ninad and Pasricha, Sudeep},
	year = {2024},
	pages = {1--14},
	file = {Hogade and Pasricha - 2024 - Game-Theoretic Deep Reinforcement Learning to Mini.pdf:/Users/henrybaker/Zotero/storage/AA2NHJ86/Hogade and Pasricha - 2024 - Game-Theoretic Deep Reinforcement Learning to Mini.pdf:application/pdf},
}

@inproceedings{luccioni_power_2024,
	address = {Rio de Janeiro Brazil},
	title = {Power {Hungry} {Processing}: {Watts} {Driving} the {Cost} of {AI} {Deployment}?},
	isbn = {9798400704505},
	shorttitle = {Power {Hungry} {Processing}},
	url = {https://dl.acm.org/doi/10.1145/3630106.3658542},
	doi = {10.1145/3630106.3658542},
	language = {en},
	urldate = {2025-01-21},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Luccioni and Jernite, Yacine and Strubell, Emma},
	month = jun,
	year = {2024},
	pages = {85--99},
	file = {Luccioni et al. - 2024 - Power Hungry Processing Watts Driving the Cost of.pdf:/Users/henrybaker/Zotero/storage/637XGJC3/Luccioni et al. - 2024 - Power Hungry Processing Watts Driving the Cost of.pdf:application/pdf},
}

@inproceedings{wilkins_hybrid_2024,
	address = {Singapore Singapore},
	title = {Hybrid {Heterogeneous} {Clusters} {Can} {Lower} the {Energy} {Consumption} of {LLM} {Inference} {Workloads}},
	isbn = {9798400704802},
	url = {https://dl.acm.org/doi/10.1145/3632775.3662830},
	doi = {10.1145/3632775.3662830},
	abstract = {Both the training and use of Large Language Models (LLMs) require large amounts of energy. Their increasing popularity, therefore, raises critical concerns regarding the energy efficiency and sustainability of data centers that host them. This paper addresses the challenge of reducing energy consumption in data centers running LLMs. We propose a hybrid data center model that uses a cost-based scheduling framework to dynamically allocate LLM tasks across hardware accelerators that differ in their energy efficiencies and computational capabilities. Specifically, our workload-aware strategy determines whether tasks are processed on energy-efficient processors or high-performance GPUs based on the number of input and output tokens in a query. Our analysis of a representative LLM dataset, finds that this hybrid strategy can reduce CPU+GPU energy consumption by 7.5\% compared to a workload-unaware baseline.},
	language = {en},
	urldate = {2025-01-21},
	booktitle = {The 15th {ACM} {International} {Conference} on {Future} and {Sustainable} {Energy} {Systems}},
	publisher = {ACM},
	author = {Wilkins, Grant and Keshav, Srinivasan and Mortier, Richard},
	month = jun,
	year = {2024},
	pages = {506--513},
	file = {Wilkins et al. - 2024 - Hybrid Heterogeneous Clusters Can Lower the Energy.pdf:/Users/henrybaker/Zotero/storage/LYY5J4NS/Wilkins et al. - 2024 - Hybrid Heterogeneous Clusters Can Lower the Energy.pdf:application/pdf},
}

@misc{stojkovic_towards_2024,
	title = {Towards {Greener} {LLMs}: {Bringing} {Energy}-{Efficiency} to the {Forefront} of {LLM} {Inference}},
	shorttitle = {Towards {Greener} {LLMs}},
	url = {http://arxiv.org/abs/2403.20306},
	doi = {10.48550/arXiv.2403.20306},
	abstract = {With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-theline GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these tradeoffs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving the way for sustainable and cost-effective LLM deployment in data center environments.},
	language = {en},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Stojkovic, Jovan and Choukse, Esha and Zhang, Chaojie and Goiri, Inigo and Torrellas, Josep},
	month = mar,
	year = {2024},
	note = {arXiv:2403.20306 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {Stojkovic et al. - 2024 - Towards Greener LLMs Bringing Energy-Efficiency t.pdf:/Users/henrybaker/Zotero/storage/BBKPBXPP/Stojkovic et al. - 2024 - Towards Greener LLMs Bringing Energy-Efficiency t.pdf:application/pdf},
}

@misc{mavromatis_computing_2024,
	title = {Computing {Within} {Limits}: {An} {Empirical} {Study} of {Energy} {Consumption} in {ML} {Training} and {Inference}},
	shorttitle = {Computing {Within} {Limits}},
	url = {http://arxiv.org/abs/2406.14328},
	doi = {10.48550/arXiv.2406.14328},
	abstract = {Machine learning (ML) has seen tremendous advancements, but its environmental footprint remains a concern. Acknowledging the growing environmental impact of ML this paper investigates Green ML, examining various model architectures and hyperparameters in both training and inference phases to identify energy-efficient practices. Our study leverages software-based power measurements for ease of replication across diverse configurations, models and datasets. In this paper, we examine multiple models and hardware configurations to identify correlations across the various measurements and metrics and key contributors to energy reduction. Our analysis offers practical guidelines for constructing sustainable ML operations, emphasising energy consumption and carbon footprint reductions while maintaining performance. As identified, short-lived profiling can quantify the long-term expected energy consumption. Moreover, model parameters can also be used to accurately estimate the expected total energy without the need for extensive experimentation.},
	language = {en},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Mavromatis, Ioannis and Katsaros, Kostas and Khan, Aftab},
	month = jun,
	year = {2024},
	note = {arXiv:2406.14328 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Mavromatis et al. - 2024 - Computing Within Limits An Empirical Study of Ene.pdf:/Users/henrybaker/Zotero/storage/CAWZVNWB/Mavromatis et al. - 2024 - Computing Within Limits An Empirical Study of Ene.pdf:application/pdf},
}

@article{barbierato_toward_2024,
	title = {Toward {Green} {AI}: {A} {Methodological} {Survey} of the {Scientific} {Literature}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	shorttitle = {Toward {Green} {AI}},
	url = {https://ieeexplore.ieee.org/document/10418137/},
	doi = {10.1109/ACCESS.2024.3360705},
	abstract = {The pervasive deployment of Deep Learning models has recently prompted apprehensions regarding their ecological footprint, owing to the exorbitant levels of energy consumption necessitated by the training and inference processes. The term ‘‘Red AI’’ is employed to denote artificial intelligence (AI) models that undergo training using resource-intensive methodologies on very large datasets. This practice can engender substantial energy usage and emissions of carbon, thereby opposing ‘‘Green AI.’’ The latter concept alludes to AI models designed for similar efficiency and reduced environmental impact. This objective is realized through the utilization of smaller datasets, less computationally intensive training techniques, or sustainable energy resources. While Red AI prioritizes accuracy and performance, Green AI emphasizes efficiency and sustainability. Given that both paradigms exhibit advantages and limitations, the debates around the topics have burgeoned in the scientific arena, delving into novel algorithms, hardware innovations, and improved data utilization techniques aimed at mitigating the ecological consequences of intricate applications such as GPT and BERT. Nevertheless, due to the relative novelty of this debate, not much effort has been dedicated yet to contextualizing the essence of Red AI and the prospects of Green AI in a coherent framework. Within this context, the present work contributes by meticulously delineating both domains through a multifaceted analysis of their causes and ramifications, described from the points of computer architectures, data structures, and algorithms. Additionally, the study reviews notable instances of study cases based on complex Red AI models. The primary contribution of this article encompasses a comprehensive survey of Red and Green AI, stemming from a selection of the literature performed by the authors, subsequently organized into distinct clusters. These clusters encompass i) articles that qualitatively or quantitatively address the issue of Red AI, identifying Green AI as a plausible remedy, ii) articles offering insights into the environmental impact associated with the deployment of extensive Deep Learning models, and iii) articles introducing the techniques underpinning Green AI, aiming at mitigating the cost of Red AI. The outcome emerging from the analysis performed by this work consists of a compromise between sustainability in contrast to the performance of AI tools. Unless the complex training and inference procedures of software models mitigate their environmental impact, it will be necessary to decrease the level of accuracy of production systems, inevitably conflicting with the objective of the major AI vendors. The outcomes of this work would be beneficial to scholars pursuing intricate Deep Learning architectures in scientific research, as well as AI enterprises struggling with the protracted training demands of commercial products within the realms of Computer Vision and Natural Language Processing.},
	language = {en},
	urldate = {2025-01-21},
	journal = {IEEE Access},
	author = {Barbierato, Enrico and Gatti, Alice},
	year = {2024},
	pages = {23989--24013},
	file = {Barbierato and Gatti - 2024 - Toward Green AI A Methodological Survey of the Sc.pdf:/Users/henrybaker/Zotero/storage/7V6ZWVIW/Barbierato and Gatti - 2024 - Toward Green AI A Methodological Survey of the Sc.pdf:application/pdf},
}

@article{verdecchia_systematic_2023,
	title = {A systematic review of {Green} {\textless}span style="font-variant:small-caps;"{\textgreater}{AI}{\textless}/span{\textgreater}},
	volume = {13},
	issn = {1942-4787, 1942-4795},
	shorttitle = {A systematic review of {Green} {\textless}span style="font-variant},
	url = {https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1507},
	doi = {10.1002/widm.1507},
	abstract = {With the ever-growing adoption of artificial intelligence (AI)-based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this article, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm-agnostic or study neural networks, and use image data. Laboratory experiments are the most common research strategy. Reported Green AI energy savings go up to 115\%, with savings over 50\% being rather common. Industrial parties are involved in Green AI studies, albeit most target academic readers. Green AI tool provisioning is scarce. As a conclusion, the Green AI research field results to have reached a considerable level of maturity. Therefore, from this review emerges that the time is suitable to adopt other Green AI research strategies, and port the numerous promising academic results to industrial practice.},
	language = {en},
	number = {4},
	urldate = {2025-01-21},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Verdecchia, Roberto and Sallou, June and Cruz, Luís},
	month = jul,
	year = {2023},
	pages = {e1507},
	file = {Verdecchia et al. - 2023 - A systematic review of Green span style=font-var.pdf:/Users/henrybaker/Zotero/storage/DCBBFSTX/Verdecchia et al. - 2023 - A systematic review of Green span style=font-var.pdf:application/pdf},
}

@inproceedings{chien_reducing_2023,
	address = {Boston MA USA},
	title = {Reducing the {Carbon} {Impact} of {Generative} {AI} {Inference} (today and in 2035)},
	isbn = {9798400702426},
	url = {https://dl.acm.org/doi/10.1145/3604930.3605705},
	doi = {10.1145/3604930.3605705},
	abstract = {Generative AI, exemplified in ChatGPT, Dall-E 2, and Stable Diffusion, are exciting new applications consuming growing quantities of computing. We study the compute, energy, and carbon impacts of generative AI inference. Using ChatGPT as an exemplar, we create a workload model and compare request direction approaches (Local, Balance, CarbonMin), assessing their power use and carbon impacts.},
	language = {en},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the 2nd {Workshop} on {Sustainable} {Computer} {Systems}},
	publisher = {ACM},
	author = {Chien, Andrew A and Lin, Liuzixuan and Nguyen, Hai and Rao, Varsha and Sharma, Tristan and Wijayawardana, Rajini},
	month = jul,
	year = {2023},
	pages = {1--7},
	file = {Chien et al. - 2023 - Reducing the Carbon Impact of Generative AI Infere.pdf:/Users/henrybaker/Zotero/storage/NS88EG9F/Chien et al. - 2023 - Reducing the Carbon Impact of Generative AI Infere.pdf:application/pdf},
}

@inproceedings{hu_characterization_2021,
	title = {Characterization and {Prediction} of {Deep} {Learning} {Workloads} in {Large}-{Scale} {GPU} {Datacenters}},
	url = {http://arxiv.org/abs/2109.01313},
	doi = {10.1145/3458817.3476223},
	abstract = {Modern GPU datacenters are critical for delivering Deep Learning (DL) models and services in both the research community and industry. When operating a datacenter, optimization of resource scheduling and management can bring significant financial benefits. Achieving this goal requires a deep understanding of the job features and user behaviors. We present a comprehensive study about the characteristics of DL jobs and resource management. First, we perform a large-scale analysis of real-world job traces from SenseTime. We uncover some interesting conclusions from the perspectives of clusters, jobs and users, which can facilitate the cluster system designs. Second, we introduce a general-purpose framework, which manages resources based on historical data. As case studies, we design (1) a Quasi-Shortest-Service-First scheduling service, which can minimize the cluster-wide average job completion time by up to 6.5×; (2) a Cluster Energy Saving service, which improves overall cluster utilization by up to 13\%.},
	language = {en},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Hu, Qinghao and Sun, Peng and Yan, Shengen and Wen, Yonggang and Zhang, Tianwei},
	month = nov,
	year = {2021},
	note = {arXiv:2109.01313 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {1--15},
	file = {Hu et al. - 2021 - Characterization and Prediction of Deep Learning W.pdf:/Users/henrybaker/Zotero/storage/744AARLF/Hu et al. - 2021 - Characterization and Prediction of Deep Learning W.pdf:application/pdf},
}

@misc{anderson_treehouse_2022,
	title = {Treehouse: {A} {Case} {For} {Carbon}-{Aware} {Datacenter} {Software}},
	shorttitle = {Treehouse},
	url = {http://arxiv.org/abs/2201.02120},
	doi = {10.48550/arXiv.2201.02120},
	abstract = {The end of Dennard scaling and the slowing of Moore’s Law has put the energy use of datacenters on an unsustainable path. Datacenters are already a signiﬁcant fraction of worldwide electricity use, with application demand scaling at a rapid rate. We argue that substantial reductions in the carbon intensity of datacenter computing are possible with a software-centric approach: by making energy and carbon visible to application developers on a ﬁne-grained basis, by modifying system APIs to make it possible to make informed trade offs between performance and carbon emissions, and by raising the level of application programming to allow for ﬂexible use of more energy efﬁcient means of compute and storage. We also lay out a research agenda for systems software to reduce the carbon footprint of datacenter computing.},
	language = {en},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Anderson, Thomas and Belay, Adam and Chowdhury, Mosharaf and Cidon, Asaf and Zhang, Irene},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02120 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
	file = {Anderson et al. - 2022 - Treehouse A Case For Carbon-Aware Datacenter Soft.pdf:/Users/henrybaker/Zotero/storage/GVCDRYLS/Anderson et al. - 2022 - Treehouse A Case For Carbon-Aware Datacenter Soft.pdf:application/pdf},
}

@misc{wang_burstgpt_2024,
	title = {{BurstGPT}: {A} {Real}-world {Workload} {Dataset} to {Optimize} {LLM} {Serving} {Systems}},
	shorttitle = {{BurstGPT}},
	url = {http://arxiv.org/abs/2401.17644},
	doi = {10.48550/arXiv.2401.17644},
	abstract = {Serving systems for Large Language Models (LLMs) are often optimized to improve quality of service (QoS) and throughput. However, due to the lack of open-sourced LLM serving workloads, these systems are frequently evaluated under unrealistic workload assumptions. Consequently, performance may degrade when these systems are deployed in real-world scenarios.},
	language = {en},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Wang, Yuxin and Chen, Yuhan and Li, Zeyu and Kang, Xueze and Tang, Zhenheng and He, Xin and Guo, Rui and Wang, Xin and Wang, Qiang and Zhou, Amelie Chi and Chu, Xiaowen},
	month = jun,
	year = {2024},
	note = {arXiv:2401.17644 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
	file = {Wang et al. - 2024 - BurstGPT A Real-world Workload Dataset to Optimiz.pdf:/Users/henrybaker/Zotero/storage/59KRYNMB/Wang et al. - 2024 - BurstGPT A Real-world Workload Dataset to Optimiz.pdf:application/pdf},
}

@article{desislavov_trends_2023,
	title = {Trends in {AI} inference energy consumption: {Beyond} the performance-vs-parameter laws of deep learning},
	volume = {38},
	issn = {22105379},
	shorttitle = {Trends in {AI} inference energy consumption},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210537923000124},
	doi = {10.1016/j.suscom.2023.100857},
	abstract = {The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we analyse relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.},
	language = {en},
	urldate = {2025-01-23},
	journal = {Sustainable Computing: Informatics and Systems},
	author = {Desislavov, Radosvet and Martínez-Plumed, Fernando and Hernández-Orallo, José},
	month = apr,
	year = {2023},
	pages = {100857},
	file = {Desislavov et al. - 2023 - Trends in AI inference energy consumption Beyond .pdf:/Users/henrybaker/Zotero/storage/SZVZ8PWS/Desislavov et al. - 2023 - Trends in AI inference energy consumption Beyond .pdf:application/pdf},
}

@inproceedings{brownlee_exploring_2021,
	address = {Madrid, Spain},
	title = {Exploring the {Accuracy} – {Energy} {Trade}-off in {Machine} {Learning}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66544-466-8},
	url = {https://ieeexplore.ieee.org/document/9474356/},
	doi = {10.1109/GI52543.2021.00011},
	abstract = {Machine learning accounts for considerable global electricity demand and resulting environmental impact, as training a large deep-learning model produces 284 000kgs of the greenhouse gas carbon dioxide. In recent years, search-based approaches have begun to explore improving software to consume less energy. Machine learning is a particularly strong candidate for this because it is possible to trade off functionality (accuracy) against energy consumption, whereas with many programs functionality is simply a pass-or-fail constraint. We use a grid search to explore hyperparameter conﬁgurations for a multilayer perceptron on ﬁve classiﬁcation data sets, considering trade-offs of classiﬁcation accuracy against training or inference energy. On one data set, we show that 77\% of energy consumption for inference can saved by reducing accuracy from 94.3\% to 93.2\%. Energy for training can also be reduced by 30-50\% with minimal loss of accuracy. We also ﬁnd that structural parameters like hidden layer size is a major driver of the energyaccuracy trade-off, but there is some evidence that non-structural hyperparameters inﬂuence the trade-off too. We also show that a search-based approach has the potential to identify these tradeoffs more efﬁciently than the grid search.},
	language = {en},
	urldate = {2025-01-23},
	booktitle = {2021 {IEEE}/{ACM} {International} {Workshop} on {Genetic} {Improvement} ({GI})},
	publisher = {IEEE},
	author = {Brownlee, Alexander E.I and Adair, Jason and Haraldsson, Saemundur O. and Jabbo, John},
	month = may,
	year = {2021},
	pages = {11--18},
	file = {Brownlee et al. - 2021 - Exploring the Accuracy – Energy Trade-off in Machi.pdf:/Users/henrybaker/Zotero/storage/T8Y2K9XB/Brownlee et al. - 2021 - Exploring the Accuracy – Energy Trade-off in Machi.pdf:application/pdf},
}

@article{garcia-martin_estimation_2019,
	title = {Estimation of energy consumption in machine learning},
	volume = {134},
	issn = {07437315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518308773},
	doi = {10.1016/j.jpdc.2019.07.007},
	abstract = {Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning.},
	language = {en},
	urldate = {2025-01-23},
	journal = {Journal of Parallel and Distributed Computing},
	author = {García-Martín, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, Håkan},
	month = dec,
	year = {2019},
	pages = {75--88},
	file = {García-Martín et al. - 2019 - Estimation of energy consumption in machine learni.pdf:/Users/henrybaker/Zotero/storage/NK3MP6YR/García-Martín et al. - 2019 - Estimation of energy consumption in machine learni.pdf:application/pdf},
}

@article{schwartz_green_2020,
	title = {Green {AI}},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3381831},
	doi = {10.1145/3381831},
	abstract = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},
	language = {en},
	number = {12},
	urldate = {2025-01-23},
	journal = {Communications of the ACM},
	author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
	month = nov,
	year = {2020},
	pages = {54--63},
	file = {Schwartz et al. - 2020 - Green AI.pdf:/Users/henrybaker/Zotero/storage/YQWKRUP2/Schwartz et al. - 2020 - Green AI.pdf:application/pdf},
}

@article{rodrigues_synergy_2018,
	title = {{SyNERGY}: {An} energy measurement and prediction framework for {Convolutional} {Neural} {Networks} on {Jetson} {TX}},
	abstract = {There is a huge demand for on-device execution of deep learning algorithms on mobile and embedded platforms. These devices present constraints on the application due to limited hardware resources and power. However, current evaluation studies in existing deep learning frameworks (for example, Caffe, Tensorﬂow, Torch and others) are limited to performance measurements of these applications on high-end CPUs and GPUs. In this work, we propose "SyNERGY" a ﬁne-grained energy measurement (that is, at speciﬁc layers) and prediction framework for deep neural networks on embedded platforms. We integrate ARM’s Streamline Performance Analyser with standard deep learning frameworks such as Caffe and CuDNNv5 to quantify the energy-use of deep convolutional neural networks on the Nvidia Jetson Tegra X1. Our measurement framework provides an accurate breakdown of actual energy consumption and performance across all layers in the neural network while our prediction framework models the energy-use in terms of target-speciﬁc performance counters such as SIMD and bus accesses and application speciﬁc parameters such as Multiply and Accumulate (MAC) counts. Our experimental results using 9 representative Deep Convolutional Neural Network shows that a multi-variable linear regression model based on hardware performance counters alone achieves an average prediction test error of 8.04 ± 5.96\% compared to actual energy measurements. Surprisingly, we ﬁnd that it is possible to reﬁne the model to predict the number of SIMD instructions and main memory accesses solely from the application’s Multiply-Accumulate (MAC) counts with an average prediction test error of 0.81 ± 0.77\% and 17.97 ± 15.29\% respectively. This alleviates the need for actual measurements giving a ﬁnal average prediction test error of 7.08 ± 5.05\% using solely the application’s MAC counts as input.},
	language = {en},
	author = {Rodrigues, Crefeda Faviola and Riley, Graham and Luján, Mikel},
	year = {2018},
	file = {Rodrigues et al. - SyNERGY An energy measurement and prediction fram.pdf:/Users/henrybaker/Zotero/storage/GP9CD79E/Rodrigues et al. - SyNERGY An energy measurement and prediction fram.pdf:application/pdf},
}

@article{chen_eyeriss_2016,
	title = {Eyeriss: {An} {Energy}-{Efficient} {Reconfigurable} {Accelerator} for {Deep} {Convolutional} {Neural} {Networks}},
	language = {en},
	author = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
	year = {2016},
	file = {Chen et al. - ISSCC 2016  SESSION 14  NEXT-GENERATION PROCESSI.pdf:/Users/henrybaker/Zotero/storage/8YF7MJ4B/Chen et al. - ISSCC 2016  SESSION 14  NEXT-GENERATION PROCESSI.pdf:application/pdf},
}

@article{cai_neuralpower_2017,
	title = {{NeuralPower} : {Predict} and {Deploy} {Energy}-{Eﬃcient} {Convolutional} {Neural} {Networks}},
	abstract = {How much energy is consumed for an inference made by a convolutional neural network (CNN)?” With the increased popularity of CNNs deployed on the wide-spectrum of platforms (from mobile devices to workstations), the answer to this question has drawn signiﬁcant attention. From lengthening battery life of mobile devices to reducing the energy bill of a datacenter, it is important to understand the energy eﬃciency of CNNs during serving for making an inference, before actually training the model. In this work, we propose NeuralPower : a layer-wise predictive framework based on sparse polynomial regression, for predicting the serving energy consumption of a CNN deployed on any GPU platform. Given the architecture of a CNN, NeuralPower provides an accurate prediction and breakdown for power and runtime across all layers in the whole network, helping machine learners quickly identify the power, runtime, or energy bottlenecks. We also propose the “energy-precision ratio” (EPR) metric to guide machine learners in selecting an energy-eﬃcient CNN architecture that better trades oﬀ the energy consumption and prediction accuracy. The experimental results show that the prediction accuracy of the proposed NeuralPower outperforms the best published model to date, yielding an improvement in accuracy of up to 68.5\%. We also assess the accuracy of predictions at the network level, by predicting the runtime, power, and energy of state-of-the-art CNN architectures, achieving an average accuracy of 88.24\% in runtime, 88.34\% in power, and 97.21\% in energy. We comprehensively corroborate the eﬀectiveness of NeuralPower as a powerful framework for machine learners by testing it on diﬀerent GPU platforms and Deep Learning software tools.},
	language = {en},
	author = {Cai, Ermao and Juan, Da-Cheng and Stamoulis, Dimitrios and Marculescu, Diana},
	year = {2017},
	file = {Cai et al. - NeuralPower  Predict and Deploy Energy-Eﬃcient Co.pdf:/Users/henrybaker/Zotero/storage/AY5ZCRJ7/Cai et al. - NeuralPower  Predict and Deploy Energy-Eﬃcient Co.pdf:application/pdf},
}

@misc{canziani_analysis_2017,
	title = {An {Analysis} of {Deep} {Neural} {Network} {Models} for {Practical} {Applications}},
	url = {http://arxiv.org/abs/1605.07678},
	doi = {10.48550/arXiv.1605.07678},
	abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the ﬁeld of computer vision, the ImageNet classiﬁcation challenge has played a major role in advancing the state-of-the-art. While accuracy ﬁgures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key ﬁndings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efﬁcient DNNs.},
	language = {en},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio},
	month = apr,
	year = {2017},
	note = {arXiv:1605.07678 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Canziani et al. - 2017 - An Analysis of Deep Neural Network Models for Prac.pdf:/Users/henrybaker/Zotero/storage/SGHEX5LQ/Canziani et al. - 2017 - An Analysis of Deep Neural Network Models for Prac.pdf:application/pdf},
}

@inproceedings{li_evaluating_2016,
	address = {Atlanta, GA, USA},
	title = {Evaluating the {Energy} {Efficiency} of {Deep} {Convolutional} {Neural} {Networks} on {CPUs} and {GPUs}},
	isbn = {978-1-5090-3936-4},
	url = {http://ieeexplore.ieee.org/document/7723730/},
	doi = {10.1109/BDCloud-SocialCom-SustainCom.2016.76},
	abstract = {In recent years convolutional neural networks (CNNs) have been successfully applied to various applications that are appropriate for deep learning, from image and video processing to speech recognition. The advancements in both hardware (e.g. more powerful GPUs) and software (e.g. deep learning models, open-source frameworks and supporting libraries) have significantly improved the accuracy and training time of CNNs. However, the high speed and accuracy are at the cost of energy consumption, which has been largely ignored in previous CNN design. With the size of data sets grows exponentially, the energy demand for training such data sets increases rapidly. It is highly desirable to design deep learning frameworks and algorithms that are both accurate and energy efficient. In this paper, we conduct a comprehensive study on the power behavior and energy efficiency of numerous well-known CNNs and training frameworks on CPUs and GPUs, and we provide a detailed workload characterization to facilitate the design of energy efficient deep learning solutions.},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {2016 {IEEE} {International} {Conferences} on {Big} {Data} and {Cloud} {Computing} ({BDCloud}), {Social} {Computing} and {Networking} ({SocialCom}), {Sustainable} {Computing} and {Communications} ({SustainCom}) ({BDCloud}-{SocialCom}-{SustainCom})},
	publisher = {IEEE},
	author = {Li, Da and Chen, Xinbo and Becchi, Michela and Zong, Ziliang},
	month = oct,
	year = {2016},
	pages = {477--484},
	file = {Li et al. - 2016 - Evaluating the Energy Efficiency of Deep Convoluti.pdf:/Users/henrybaker/Zotero/storage/YSFMHHDR/Li et al. - 2016 - Evaluating the Energy Efficiency of Deep Convoluti.pdf:application/pdf},
}

@misc{stojkovic_tapas_2025,
	title = {{TAPAS}: {Thermal}- and {Power}-{Aware} {Scheduling} for {LLM} {Inference} in {Cloud} {Platforms}},
	shorttitle = {{TAPAS}},
	url = {http://arxiv.org/abs/2501.02600},
	doi = {10.48550/arXiv.2501.02600},
	abstract = {We make the following main contributions: • Characterization of thermal/power properties of GPU workloads and their behavior at production scale. • TAPAS, the first thermal- and power-aware scheduling scheme for LLM inference systems. • A thorough evaluation of TAPAS in a GPU cluster using large-scale production traces.},
	language = {en},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Stojkovic, Jovan and Zhang, Chaojie and Goiri, Íñigo and Choukse, Esha and Qiu, Haoran and Fonseca, Rodrigo and Torrellas, Josep and Bianchini, Ricardo},
	month = jan,
	year = {2025},
	note = {arXiv:2501.02600 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Stojkovic et al. - 2025 - TAPAS Thermal- and Power-Aware Scheduling for LLM.pdf:/Users/henrybaker/Zotero/storage/TQAEPK7R/Stojkovic et al. - 2025 - TAPAS Thermal- and Power-Aware Scheduling for LLM.pdf:application/pdf},
}

@misc{luccioni_counting_2023,
	title = {Counting {Carbon}: {A} {Survey} of {Factors} {Influencing} the {Emissions} of {Machine} {Learning}},
	shorttitle = {Counting {Carbon}},
	url = {http://arxiv.org/abs/2302.08476},
	doi = {10.48550/arXiv.2302.08476},
	abstract = {Machine learning (ML) requires using energy to carry out computations during the model training process. The generation of this energy comes with an environmental cost in terms of greenhouse gas emissions, depending on quantity used and the energy source. Existing research on the environmental impacts of ML has been limited to analyses covering a small number of models and does not adequately represent the diversity of ML models and tasks. In the current study, we present a survey of the carbon emissions of 95 ML models across time and different tasks in natural language processing and computer vision. We analyze them in terms of the energy sources used, the amount of CO2 emissions produced, how these emissions evolve across time and how they relate to model performance. We conclude with a discussion regarding the carbon footprint of our field and propose the creation of a centralized repository for reporting and tracking these emissions.},
	language = {en},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Luccioni, Alexandra Sasha and Hernandez-Garcia, Alex},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08476 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Luccioni and Hernandez-Garcia - 2023 - Counting Carbon A Survey of Factors Influencing t.pdf:/Users/henrybaker/Zotero/storage/4GEH34JU/Luccioni and Hernandez-Garcia - 2023 - Counting Carbon A Survey of Factors Influencing t.pdf:application/pdf},
}

@article{strubell_energy_2020,
	title = {Energy and {Policy} {Considerations} for {Modern} {Deep} {Learning} {Research}},
	volume = {34},
	copyright = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7123},
	doi = {10.1609/aaai.v34i09.7123},
	abstract = {The ﬁeld of artiﬁcial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both ﬁnancially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate ﬁnancial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we brieﬂy summarize our ﬁndings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artiﬁcial intelligence community.},
	language = {en},
	number = {09},
	urldate = {2025-01-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = apr,
	year = {2020},
	pages = {13693--13696},
	file = {Strubell et al. - 2020 - Energy and Policy Considerations for Modern Deep L.pdf:/Users/henrybaker/Zotero/storage/3L5RUZZR/Strubell et al. - 2020 - Energy and Policy Considerations for Modern Deep L.pdf:application/pdf},
}

@misc{tripp_measuring_2024,
	title = {Measuring the {Energy} {Consumption} and {Efficiency} of {Deep} {Neural} {Networks}: {An} {Empirical} {Analysis} and {Design} {Recommendations}},
	shorttitle = {Measuring the {Energy} {Consumption} and {Efficiency} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2403.08151},
	doi = {10.48550/arXiv.2403.08151},
	abstract = {Addressing the so-called “Red-AI” trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures. We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network “shapes”, and 14 depths on both CPU and GPU hardware collected using node-level watt-meters. This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects. We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy. Our analysis also uncovers a surprising, hardwaremediated non-linear relationship between energy efficiency and network design, challenging the assumption that reducing the number of parameters or FLOPs is the best way to achieve greater energy efficiency. Highlighting the need for cache-considerate algorithm development, we suggest a combined approach to energy efficient network, algorithm, and hardware design. This work contributes to the fields of sustainable computing and Green AI, offering practical guidance for creating more energy-efficient neural networks and promoting sustainable AI.},
	language = {en},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Tripp, Charles Edison and Perr-Sauer, Jordan and Gafur, Jamil and Nag, Amabarish and Purkayastha, Avi and Zisman, Sagi and Bensen, Erik A.},
	month = mar,
	year = {2024},
	note = {arXiv:2403.08151 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Tripp et al. - 2024 - Measuring the Energy Consumption and Efficiency of.pdf:/Users/henrybaker/Zotero/storage/6H5WRJ89/Tripp et al. - 2024 - Measuring the Energy Consumption and Efficiency of.pdf:application/pdf},
}

@inproceedings{wang_energy_2023,
	title = {Energy and {Carbon} {Considerations} of {Fine}-{Tuning} {BERT}},
	url = {http://arxiv.org/abs/2311.10267},
	doi = {10.18653/v1/2023.findings-emnlp.607},
	abstract = {Despite the popularity of the ‘pre-train then fine-tune’ paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of finetuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of finetuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their finetuning energy efficiency.},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	author = {Wang, Xiaorong and Na, Clara and Strubell, Emma and Friedler, Sorelle and Luccioni, Sasha},
	year = {2023},
	note = {arXiv:2311.10267 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {9058--9069},
	file = {Wang et al. - 2023 - Energy and Carbon Considerations of Fine-Tuning BE.pdf:/Users/henrybaker/Zotero/storage/X4KP6889/Wang et al. - 2023 - Energy and Carbon Considerations of Fine-Tuning BE.pdf:application/pdf},
}

@inproceedings{da_silva_barros_scheduling_2024,
	address = {Gotland Sweden},
	title = {Scheduling {Machine} {Learning} {Compressible} {Inference} {Tasks} with {Limited} {Energy} {Budget}},
	isbn = {9798400717932},
	url = {https://dl.acm.org/doi/10.1145/3673038.3673106},
	doi = {10.1145/3673038.3673106},
	abstract = {Advancements in cloud computing have boosted Machine Learning as a Service (MLaaS), highlighting the challenge of scheduling tasks under latency and deadline constraints. Neural network compression offers the latency and energy consumption reduction in data centers, aligning with efforts to minimize cloud computing’s carbon footprint, despite some accuracy loss.},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the 53rd {International} {Conference} on {Parallel} {Processing}},
	publisher = {ACM},
	author = {Da Silva Barros, Tiago and Ferre, Davide and Giroire, Frederic and Aparicio-Pardo, Ramon and Perennes, Stephane},
	month = aug,
	year = {2024},
	pages = {961--970},
	file = {Da Silva Barros et al. - 2024 - Scheduling Machine Learning Compressible Inference.pdf:/Users/henrybaker/Zotero/storage/7LYU4C69/Da Silva Barros et al. - 2024 - Scheduling Machine Learning Compressible Inference.pdf:application/pdf},
}

@misc{husom_price_2024,
	title = {The {Price} of {Prompting}: {Profiling} {Energy} {Use} in {Large} {Language} {Models} {Inference}},
	shorttitle = {The {Price} of {Prompting}},
	url = {http://arxiv.org/abs/2407.16893},
	doi = {10.48550/arXiv.2407.16893},
	abstract = {In the rapidly evolving realm of artificial intelligence, deploying large language models (LLMs) poses increasingly pressing computational and environmental challenges. This paper introduces MELODI – Monitoring Energy Levels and Optimization for Data-driven Inference – a multifaceted framework crafted to monitor and analyze the energy consumed during LLM inference processes. MELODI enables detailed observations of power consumption dynamics and facilitates the creation of a comprehensive dataset reflective of energy efficiency across varied deployment scenarios. The dataset, generated using MELODI, encompasses a broad spectrum of LLM deployment frameworks, multiple language models, and extensive prompt datasets, enabling a comparative analysis of energy use. Using the dataset, we investigate how prompt attributes, including length and complexity, correlate with energy expenditure. Our findings indicate substantial disparities in energy efficiency, suggesting ample scope for optimization and adoption of sustainable measures in LLM deployment. Our contribution lies not only in the MELODI framework but also in the novel dataset, a resource that can be expanded by other researchers. Thus, MELODI is a foundational tool and dataset for advancing research into energy-conscious LLM deployment, steering the field toward a more sustainable future. The released code and dataset are available at https://github.com/ejhusom/MELODI.},
	language = {en},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Husom, Erik Johannes and Goknil, Arda and Shar, Lwin Khin and Sen, Sagar},
	month = jul,
	year = {2024},
	note = {arXiv:2407.16893 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Computation and Language},
	file = {Husom et al. - 2024 - The Price of Prompting Profiling Energy Use in La.pdf:/Users/henrybaker/Zotero/storage/WYDCXV8P/Husom et al. - 2024 - The Price of Prompting Profiling Energy Use in La.pdf:application/pdf},
}

@inproceedings{alizadeh_green_2024,
	title = {Green {AI}: {A} {Preliminary} {Empirical} {Study} on {Energy} {Consumption} in {DL} {Models} {Across} {Different} {Runtime} {Infrastructures}},
	shorttitle = {Green {AI}},
	url = {http://arxiv.org/abs/2402.13640},
	doi = {10.1145/3644815.3644967},
	abstract = {Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime infrastructures responsible for executing trained models on target hardware, managing memory, data transfers, and multi-accelerator execution, if applicable. Additionally, it is a common practice to deploy pre-trained models on environments distinct from their native development settings. This led to the introduction of interchange formats such as ONNX, which includes its runtime infrastructure, and ONNX Runtime, which work as standard formats that can be used across diverse DL frameworks and languages. Even though these runtime infrastructures have a great impact on inference performance, no previous paper has investigated their energy efficiency. In this study, we monitor the energy consumption and inference time in the runtime infrastructures of three wellknown DL frameworks as well as ONNX, using three various DL models. To have nuance in our investigation, we also examine the impact of using different execution providers. We find out that the performance and energy efficiency of DL are difficult to predict. One framework, MXNet, outperforms both PyTorch and TensorFlow for the computer vision models using batch size 1, due to efficient GPU usage and thus low CPU usage. However, batch size 64 makes PyTorch and MXNet practically indistinguishable, while TensorFlow is outperformed consistently. For BERT, PyTorch exhibits the best performance. Converting the models to ONNX yields significant performance improvements in the majority of cases. Finally, in our preliminary investigation of execution providers, we observe that TensorRT always outperforms CUDA.},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the {IEEE}/{ACM} 3rd {International} {Conference} on {AI} {Engineering} - {Software} {Engineering} for {AI}},
	author = {Alizadeh, Negar and Castor, Fernando},
	month = apr,
	year = {2024},
	note = {arXiv:2402.13640 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	pages = {134--139},
	file = {Alizadeh and Castor - 2024 - Green AI A Preliminary Empirical Study on Energy .pdf:/Users/henrybaker/Zotero/storage/RNLHYBAF/Alizadeh and Castor - 2024 - Green AI A Preliminary Empirical Study on Energy .pdf:application/pdf},
}

@misc{wilkins_offline_2024,
	title = {Offline {Energy}-{Optimal} {LLM} {Serving}: {Workload}-{Based} {Energy} {Models} for {LLM} {Inference} on {Heterogeneous} {Systems}},
	shorttitle = {Offline {Energy}-{Optimal} {LLM} {Serving}},
	url = {http://arxiv.org/abs/2407.04014},
	doi = {10.48550/arXiv.2407.04014},
	abstract = {The rapid adoption of large language models (LLMs) has led to significant advances in natural language processing and text generation. However, the energy consumed through LLM model inference remains a major challenge for sustainable AI deployment. To address this problem, we model the workload-dependent energy consumption and runtime of LLM inference tasks on heterogeneous GPU-CPU systems. By conducting an extensive characterization study of several state-of-the-art LLMs and analyzing their energy and runtime behavior across different magnitudes of input prompts and output text, we develop accurate (𝑅2 {\textgreater} 0.96) energy and runtime models for each LLM. We employ these models to explore an offline, energy-optimal LLM workload scheduling framework. Through a case study, we demonstrate the advantages of energy and accuracy aware scheduling compared to existing best practices.},
	language = {en},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Wilkins, Grant and Keshav, Srinivasan and Mortier, Richard},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04014 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Wilkins et al. - 2024 - Offline Energy-Optimal LLM Serving Workload-Based.pdf:/Users/henrybaker/Zotero/storage/2SIQSN7Q/Wilkins et al. - 2024 - Offline Energy-Optimal LLM Serving Workload-Based.pdf:application/pdf},
}

@article{sinha_energy_2022,
	title = {Energy {Efficiency} of {Quantized} {Neural} {Networks} in {Medical} {Imaging}},
	abstract = {The main goal of this paper is to compare the energy efficiency of quantized neural networks to perform medical image analysis on different processors and neural network architectures. Deep neural networks have demonstrated outstanding performance in medical image analysis but require high computation and power usage. In our work, we review the power usage and temperature of processors when running Resnet and UNet architectures to perform image classification and segmentation respectively. We compare Edge TPU, Jetson Nano, Apple M1, Nvidia Quadro P6000 and Nvidia A6000 to infer using full-precision FP32 and quantized INT8 models. The results will be useful for designers and implementers of medical imaging AI on hand-held or edge computing devices.},
	language = {en},
	author = {Sinha, Priyanshu and Tummala, Sai Sreya and Purkayastha, Saptarshi and Gichoya, Judy W},
	year = {2022},
	file = {Sinha et al. - Energy Efficiency of Quantized Neural Networks in .pdf:/Users/henrybaker/Zotero/storage/B3PSEJS7/Sinha et al. - Energy Efficiency of Quantized Neural Networks in .pdf:application/pdf},
}

@misc{yang_double-exponential_2024,
	title = {Double-{Exponential} {Increases} in {Inference} {Energy}: {The} {Cost} of the {Race} for {Accuracy}},
	shorttitle = {Double-{Exponential} {Increases} in {Inference} {Energy}},
	url = {http://arxiv.org/abs/2412.09731},
	doi = {10.48550/arXiv.2412.09731},
	abstract = {Deep learning models in computer vision have achieved significant success but pose increasing concerns about energy consumption and sustainability. Despite these concerns, there is a lack of comprehensive understanding of their energy efficiency during inference. In this study, we conduct a comprehensive analysis of the inference energy consumption of 1,200 ImageNet classification models—the largest evaluation of its kind to date. Our findings reveal a steep diminishing return in accuracy gains relative to the increase in energy usage, highlighting sustainability concerns in the pursuit of marginal improvements. We identify key factors contributing to energy consumption and demonstrate methods to improve energy efficiency. To promote more sustainable AI practices, we introduce an energy efficiency scoring system and develop an interactive web application that allows users to compare models based on accuracy and energy consumption. By providing extensive empirical data and practical tools, we aim to facilitate informed decision-making and encourage collaborative efforts in developing energy-efficient AI technologies.},
	language = {en},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Yang, Zeyu and Adamek, Karel and Armour, Wesley},
	month = dec,
	year = {2024},
	note = {arXiv:2412.09731 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Yang et al. - 2024 - Double-Exponential Increases in Inference Energy .pdf:/Users/henrybaker/Zotero/storage/LHB5N42J/Yang et al. - 2024 - Double-Exponential Increases in Inference Energy .pdf:application/pdf},
}

@article{henderson_towards_2020,
	title = {Towards the {Systematic} {Reporting} of the {Energy} and {Carbon} {Footprints} of {Machine} {Learning}},
	abstract = {Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy eﬃcient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy eﬃcient algorithms.},
	language = {en},
	author = {Henderson, Peter and Hu, Jieru and Romoﬀ, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
	year = {2020},
	file = {Henderson et al. - Towards the Systematic Reporting of the Energy and.pdf:/Users/henrybaker/Zotero/storage/GBA8ZFTP/Henderson et al. - Towards the Systematic Reporting of the Energy and.pdf:application/pdf},
}

@misc{fischer_energy_2023,
	title = {Energy {Efficiency} {Considerations} for {Popular} {AI} {Benchmarks}},
	url = {http://arxiv.org/abs/2304.08359},
	doi = {10.48550/arXiv.2304.08359},
	abstract = {Advances in artiﬁcial intelligence need to become more resource-aware and sustainable. This requires clear assessment and reporting of energy efﬁciency trade-offs, like sacriﬁcing fast running time for higher predictive performance. While ﬁrst methods for investigating efﬁciency have been proposed, we still lack comprehensive results for popular methods and data sets. In this work, we attempt to ﬁll this information gap by providing empiric insights for popular AI benchmarks, with a total of 100 experiments. Our ﬁndings are evidence of how different data sets all have their own efﬁciency landscape, and show that methods can be more or less likely to act efﬁciently.},
	language = {en},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Fischer, Raphael and Jakobs, Matthias and Morik, Katharina},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance},
	file = {Fischer et al. - 2023 - Energy Efficiency Considerations for Popular AI Be.pdf:/Users/henrybaker/Zotero/storage/J8U7H7VC/Fischer et al. - 2023 - Energy Efficiency Considerations for Popular AI Be.pdf:application/pdf},
}

@inproceedings{georgiou_green_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Green {AI}: do deep learning frameworks have different costs?},
	isbn = {978-1-4503-9221-1},
	shorttitle = {Green {AI}},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510221},
	doi = {10.1145/3510003.3510221},
	language = {en},
	urldate = {2025-01-31},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Georgiou, Stefanos and Kechagia, Maria and Sharma, Tushar and Sarro, Federica and Zou, Ying},
	month = may,
	year = {2022},
	pages = {1082--1094},
	file = {Georgiou et al. - 2022 - Green AI do deep learning frameworks have differe.pdf:/Users/henrybaker/Zotero/storage/X87CBCPC/Georgiou et al. - 2022 - Green AI do deep learning frameworks have differe.pdf:application/pdf},
}

@misc{liang_holistic_2023,
	title = {Holistic {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2211.09110},
	doi = {10.48550/arXiv.2211.09110},
	abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what’s missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios to the extent possible (87.5\% of the time), ensuring that metrics beyond accuracy don’t fall to the wayside, and that trade-offs across models and metrics are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to more deeply analyze specific aspects (e.g. knowledge, reasoning, memorization/copyright, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, including 21 scenarios that were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on a set of core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings concerning the interplay between different scenarios, metrics, and models. For full transparency, we release all raw model prompts and completions publicly1 for further analysis, as well as a general modular toolkit for easily adding new scenarios, models, metrics, and prompting strategies.2 We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	language = {en},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	month = oct,
	year = {2023},
	note = {arXiv:2211.09110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Liang et al. - 2023 - Holistic Evaluation of Language Models.pdf:/Users/henrybaker/Zotero/storage/B9QNFDWD/Liang et al. - 2023 - Holistic Evaluation of Language Models.pdf:application/pdf},
}

@misc{samsi_words_2023,
	title = {From {Words} to {Watts}: {Benchmarking} the {Energy} {Costs} of {Large} {Language} {Model} {Inference}},
	shorttitle = {From {Words} to {Watts}},
	url = {http://arxiv.org/abs/2310.03003},
	doi = {10.48550/arXiv.2310.03003},
	abstract = {Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs—despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.},
	language = {en},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Samsi, Siddharth and Zhao, Dan and McDonald, Joseph and Li, Baolin and Michaleas, Adam and Jones, Michael and Bergeron, William and Kepner, Jeremy and Tiwari, Devesh and Gadepally, Vijay},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03003 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Samsi et al. - 2023 - From Words to Watts Benchmarking the Energy Costs.pdf:/Users/henrybaker/Zotero/storage/VU9RPTSR/Samsi et al. - 2023 - From Words to Watts Benchmarking the Energy Costs.pdf:application/pdf},
}

@incollection{koprinska_unified_2023,
	address = {Cham},
	title = {A {Unified} {Framework} for {Assessing} {Energy} {Efficiency} of {Machine} {Learning}},
	volume = {1752},
	isbn = {978-3-031-23617-4 978-3-031-23618-1},
	url = {https://link.springer.com/10.1007/978-3-031-23618-1_3},
	abstract = {State-of-the-art machine learning (ML) systems show exceptional qualitative performance, but can also have a negative impact on society. With regard to global climate change, the question of resource consumption and sustainability becomes more and more urgent. The enormous energy footprint of single ML applications and experiments was recently investigated. However, environment-aware users require a uniﬁed framework to assess, compare, and report the eﬃciency and performance trade-oﬀ of diﬀerent methods and models. In this work we propose novel eﬃciency aggregation, indexing, and rating procedures for ML applications. To this end, we devise a set of metrics that allow for a holistic view, taking both task type, abstract model, software, and hardware into account. As a result, ML systems become comparable even across diﬀerent execution environments. Inspired by the EU’s energy label system, we also introduce a concept for visually communicating eﬃciency information to the public in a comprehensible way. We apply our methods to over 20 SOTA models on a range of hardware architectures, giving an overview of the modern ML eﬃciency landscape.},
	language = {en},
	urldate = {2025-02-03},
	booktitle = {Machine {Learning} and {Principles} and {Practice} of {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Nature Switzerland},
	author = {Fischer, Raphael and Jakobs, Matthias and Mücke, Sascha and Morik, Katharina},
	editor = {Koprinska, Irena and Mignone, Paolo and Guidotti, Riccardo and Jaroszewicz, Szymon and Fröning, Holger and Gullo, Francesco and Ferreira, Pedro M. and Roqueiro, Damian and Ceddia, Gaia and Nowaczyk, Slawomir and Gama, João and Ribeiro, Rita and Gavaldà, Ricard and Masciari, Elio and Ras, Zbigniew and Ritacco, Ettore and Naretto, Francesca and Theissler, Andreas and Biecek, Przemyslaw and Verbeke, Wouter and Schiele, Gregor and Pernkopf, Franz and Blott, Michaela and Bordino, Ilaria and Danesi, Ivan Luciano and Ponti, Giovanni and Severini, Lorenzo and Appice, Annalisa and Andresini, Giuseppina and Medeiros, Ibéria and Graça, Guilherme and Cooper, Lee and Ghazaleh, Naghmeh and Richiardi, Jonas and Saldana, Diego and Sechidis, Konstantinos and Canakoglu, Arif and Pido, Sara and Pinoli, Pietro and Bifet, Albert and Pashami, Sepideh},
	year = {2023},
	doi = {10.1007/978-3-031-23618-1_3},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {39--54},
	file = {Fischer et al. - 2023 - A Unified Framework for Assessing Energy Efficienc.pdf:/Users/henrybaker/Zotero/storage/VBAN6XVZ/Fischer et al. - 2023 - A Unified Framework for Assessing Energy Efficienc.pdf:application/pdf},
}

@inproceedings{li_miso_2022,
	title = {{MISO}: {Exploiting} {Multi}-{Instance} {GPU} {Capability} on {Multi}-{Tenant} {Systems} for {Machine} {Learning}},
	shorttitle = {{MISO}},
	url = {http://arxiv.org/abs/2207.11428},
	doi = {10.1145/3542929.3563510},
	abstract = {GPU technology has been improving at an expedited pace in terms of size and performance, empowering HPC and AI/ML researchers to advance the scientific discovery process. However, this also leads to inefficient resource usage, as most GPU workloads, including complicated AI/ML models, are not able to utilize the GPU resources to their fullest extent – encouraging support for GPU multi-tenancy. We propose MISO 1, a technique to exploit the Multi-Instance GPU (MIG) capability on the latest NVIDIA datacenter GPUs (e.g., A100, H100) to dynamically partition GPU resources among co-located jobs. MISO’s key insight is to use the lightweight, more flexible Multi-Process Service (MPS) capability to predict the best MIG partition allocation for different jobs, without incurring the overhead of implementing them during exploration. Due to its ability to utilize GPU resources more efficiently, MISO achieves 49\% and 16\% lower average job completion time than the unpartitioned and optimal static GPU partition schemes, respectively.},
	language = {en},
	urldate = {2025-02-03},
	booktitle = {Proceedings of the 13th {Symposium} on {Cloud} {Computing}},
	author = {Li and Patel, Tirthak and Samsi, Siddarth and Gadepally, Vijay and Tiwari, Devesh},
	month = nov,
	year = {2022},
	note = {arXiv:2207.11428 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	pages = {173--189},
	file = {Li et al. - 2022 - MISO Exploiting Multi-Instance GPU Capability on .pdf:/Users/henrybaker/Zotero/storage/UDY4WHC8/Li et al. - 2022 - MISO Exploiting Multi-Instance GPU Capability on .pdf:application/pdf},
}

@misc{luccioni_energy_2024,
	title = {Energy {Scores} for {AI} {Models}},
	url = {https://huggingface.co/blog/sasha/energy-star-ai-proposal},
	abstract = {A Blog post by Sasha Luccioni on Hugging Face},
	urldate = {2025-02-19},
	author = {Luccioni, Sasha},
	year = {2024},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/ACCVTKB8/energy-star-ai-proposal.html:text/html},
}

@misc{ebert_ai_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {{AI}, {Climate}, and {Regulation}: {From} {Data} {Centers} to the {AI} {Act}},
	shorttitle = {{AI}, {Climate}, and {Regulation}},
	url = {https://papers.ssrn.com/abstract=4980340},
	doi = {10.2139/ssrn.4980340},
	abstract = {We live in a world that is experiencing an unprecedented boom of AI applications that increasingly penetrate and enhance all sectors of private and public life, from education, media, medicine, and mobility to the industrial and professional workspace, and-potentially particularly consequentially-robotics. As this world is simultaneously grappling with climate change, the climate and environmental implications of the development and use of AI have become an important subject of public and academic debate. In this paper, we aim to provide guidance on the climate-related regulation for data centers and AI specifically, and discuss how to operationalize these requirements. We also highlight challenges and room for improvement, and make a number of policy proposals to this end. In particular, we propose a specific interpretation of the AI Act to bring reporting on the previously unadressed energy consumption from AI inferences back into the scope. We also find that the AI Act fails to address indirect greenhouse gas emissions from AI applications. Furthermore, for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level. We also argue for an interpretation of the AI Act that includes environmental concerns in the mandatory risk assessment (sustainability risk assessment, SIA), and provide guidance on its operationalization. The EU data center regulation proves to be a good first step but requires further development by including binding renewable energy and efficiency targets for data centers. Overall, we make twelve concrete policy proposals, in four main areas: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability Mechanisms; and Future Far-Reaching Measures beyond Transparency.},
	language = {en},
	urldate = {2025-02-20},
	publisher = {Social Science Research Network},
	author = {Ebert, Kai and Alder, Nicolas and Herbrich, Ralf and Hacker, Philipp},
	month = oct,
	year = {2024},
	keywords = {sustainability, AI Act, data center regulation, Energy Efficiency Directive, energy reporting, sustainability risk assessment},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/M7KBT3BD/Ebert et al. - 2024 - AI, Climate, and Regulation From Data Centers to .pdf:application/pdf},
}

@misc{poddar_towards_2025,
	title = {Towards {Sustainable} {NLP}: {Insights} from {Benchmarking} {Inference} {Energy} in {Large} {Language} {Models}},
	shorttitle = {Towards {Sustainable} {NLP}},
	url = {http://arxiv.org/abs/2502.05610},
	doi = {10.48550/arXiv.2502.05610},
	abstract = {Large language models (LLMs) are increasingly recognized for their exceptional generative capabilities and versatility across various tasks. However, the high inference costs associated with these models have not received adequate attention, particularly when compared to the focus on training costs in existing research. In response to this gap, our study conducts a comprehensive benchmarking of LLM inference energy across a wide range of NLP tasks, where we analyze the impact of different models, tasks, prompts, and system-related factors on inference energy. Specifically, our experiments reveal several interesting insights, including strong correlation of inference energy with output token length and response time. Also, we find that quantization and optimal batch sizes, along with targeted prompt phrases, can significantly reduce energy usage. This study is the first to thoroughly benchmark LLM inference across such a diverse range of aspects, providing insights and offering several recommendations for improving energy efficiency in model deployment.},
	language = {en},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Poddar, Soham and Koley, Paramita and Misra, Janardan and Ganguly, Niloy and Ghosh, Saptarshi},
	month = feb,
	year = {2025},
	note = {arXiv:2502.05610 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Poddar et al. - 2025 - Towards Sustainable NLP Insights from Benchmarkin.pdf:/Users/henrybaker/Zotero/storage/4EJ69NJU/Poddar et al. - 2025 - Towards Sustainable NLP Insights from Benchmarkin.pdf:application/pdf},
}

@misc{nik_energy-conscious_2025,
	title = {Energy-{Conscious} {LLM} {Decoding}: {Impact} of {Text} {Generation} {Strategies} on {GPU} {Energy} {Consumption}},
	shorttitle = {Energy-{Conscious} {LLM} {Decoding}},
	url = {http://arxiv.org/abs/2502.11723},
	doi = {10.48550/arXiv.2502.11723},
	abstract = {Decoding strategies significantly influence the quality and diversity of the generated texts in large language models (LLMs), yet their impact on computational resource consumption, particularly GPU energy usage, is insufficiently studied. This paper investigates the relationship between text generation decoding methods and energy efficiency, focusing on the trade-off between generation quality and GPU energy consumption across diverse tasks and decoding configurations. By benchmarking multiple strategies across different text generation tasks, such as Translation, Code Summarization, and Math Problem Solving, we reveal how selecting appropriate decoding techniques with their tuned hyperparameters affects text quality and has measurable implications for resource utilization, emphasizing the need for balanced optimization. To the best of our knowledge, this study is among the first to explore decoding strategies in LLMs through the lens of energy consumption, offering actionable insights for designing resource-aware applications that maintain high-quality text generation.},
	urldate = {2025-03-27},
	publisher = {arXiv},
	author = {Nik, Alireza and Riegler, Michael A. and Halvorsen, Pål},
	month = feb,
	year = {2025},
	note = {arXiv:2502.11723 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/2PICFX9Q/Nik et al. - 2025 - Energy-Conscious LLM Decoding Impact of Text Gene.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/UDPPATV3/2502.html:text/html},
}

@misc{maliakel_investigating_2025,
	title = {Investigating {Energy} {Efficiency} and {Performance} {Trade}-offs in {LLM} {Inference} {Across} {Tasks} and {DVFS} {Settings}},
	url = {http://arxiv.org/abs/2501.08219},
	doi = {10.48550/arXiv.2501.08219},
	abstract = {Large language models (LLMs) have shown significant improvements in many natural language processing (NLP) tasks, accelerating their rapid adoption across many industries. These models are resource-intensive, requiring extensive computational resources both during training and inference, leading to increased energy consumption and negative environmental impact. As their adoption accelerates, the sustainability of LLMs has become a critical issue, necessitating strategies to optimize their runtime efficiency without compromising performance. Hence, it is imperative to identify the parameters that significantly influence the performance and energy efficiency of LLMs. To that end, in this work, we investigate the effect of important parameters on the performance and energy efficiency of LLMs during inference and examine their trade-offs. First, we analyze how different types of models with varying numbers of parameters and architectures perform on tasks like text generation, question answering, and summarization by benchmarking LLMs such as Falcon-7B, Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study input and output sequence characteristics such as sequence length concerning energy consumption, performance, and throughput. Finally, we explore the impact of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency Scaling (DVFS), on the models' latency and energy efficiency. Our extensive benchmarking and statistical analysis reveal many interesting findings, uncovering how specific optimizations can reduce energy consumption while maintaining throughput and accuracy. This study provides actionable insights for researchers and practitioners to design energy-efficient LLM inference systems.},
	urldate = {2025-03-27},
	publisher = {arXiv},
	author = {Maliakel, Paul Joe and Ilager, Shashikant and Brandic, Ivona},
	month = jan,
	year = {2025},
	note = {arXiv:2501.08219 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/VG33REYQ/Maliakel et al. - 2025 - Investigating Energy Efficiency and Performance Tr.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/HM3ICYTS/2501.html:text/html},
}

@misc{wu_unveiling_2025,
	title = {Unveiling {Environmental} {Impacts} of {Large} {Language} {Model} {Serving}: {A} {Functional} {Unit} {View}},
	shorttitle = {Unveiling {Environmental} {Impacts} of {Large} {Language} {Model} {Serving}},
	url = {http://arxiv.org/abs/2502.11256},
	doi = {10.48550/arXiv.2502.11256},
	abstract = {Large language models (LLMs) offer powerful capabilities but come with significant environmental costs, particularly in carbon emissions. Existing studies benchmark these emissions but lack a standardized basis for comparison across models. To address this, we introduce the concept of a functional unit (FU) and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through case studies on model size, quantization, and hardware, we uncover key trade-offs in sustainability. Our findings highlight the potential for reducing carbon emissions by optimizing model selection, deployment strategies, and hardware choices, paving the way for more sustainable AI infrastructure.},
	urldate = {2025-03-30},
	publisher = {arXiv},
	author = {Wu and Hua, Inez and Ding, Yi},
	month = feb,
	year = {2025},
	note = {arXiv:2502.11256 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Hardware Architecture},
	file = {Preprint PDF:/Users/henrybaker/Zotero/storage/YLCD359K/Wu et al. - 2025 - Unveiling Environmental Impacts of Large Language .pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/DAILZLH9/2502.html:text/html},
}

@misc{fernandez_framework_2023,
	title = {The {Framework} {Tax}: {Disparities} {Between} {Inference} {Efficiency} in {NLP} {Research} and {Deployment}},
	shorttitle = {The {Framework} {Tax}},
	url = {http://arxiv.org/abs/2302.06117},
	doi = {10.48550/arXiv.2302.06117},
	abstract = {Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the {\textbackslash}textit\{framework tax\}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.},
	urldate = {2025-03-30},
	publisher = {arXiv},
	author = {Fernandez, Jared and Kahn, Jacob and Na, Clara and Bisk, Yonatan and Strubell, Emma},
	month = dec,
	year = {2023},
	note = {arXiv:2302.06117 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/henrybaker/Zotero/storage/DP83RIQG/Fernandez et al. - 2023 - The Framework Tax Disparities Between Inference E.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/QY6JV32L/2302.html:text/html},
}

@misc{dehghani_efficiency_2022,
	title = {The {Efficiency} {Misnomer}},
	url = {http://arxiv.org/abs/2110.12894},
	doi = {10.48550/arXiv.2110.12894},
	abstract = {Model efficiency is a critical aspect of developing and deploying machine learning models. Inference time and latency directly affect the user experience, and some applications have hard requirements. In addition to inference costs, model training also have direct financial and environmental impacts. Although there are numerous well-established metrics (cost indicators) for measuring model efficiency, researchers and practitioners often assume that these metrics are correlated with each other and report only few of them. In this paper, we thoroughly discuss common cost indicators, their advantages and disadvantages, and how they can contradict each other. We demonstrate how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models. We further present suggestions to improve reporting of efficiency metrics.},
	urldate = {2025-03-30},
	publisher = {arXiv},
	author = {Dehghani, Mostafa and Arnab, Anurag and Beyer, Lucas and Vaswani, Ashish and Tay, Yi},
	month = mar,
	year = {2022},
	note = {arXiv:2110.12894 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/henrybaker/Zotero/storage/TBQJGURT/Dehghani et al. - 2022 - The Efficiency Misnomer.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/6WL6TEK8/2110.html:text/html},
}

@misc{fernandez_hardware_2024,
	title = {Hardware {Scaling} {Trends} and {Diminishing} {Returns} in {Large}-{Scale} {Distributed} {Training}},
	url = {http://arxiv.org/abs/2411.13055},
	doi = {10.48550/arXiv.2411.13055},
	abstract = {Dramatic increases in the capabilities of neural network models in recent years are driven by scaling model size, training data, and corresponding computational resources. To develop the exceedingly large networks required in modern applications, such as large language models (LLMs), model training is distributed across tens of thousands of hardware accelerators (e.g. GPUs), requiring orchestration of computation and communication across large computing clusters. In this work, we demonstrate that careful consideration of hardware configuration and parallelization strategy is critical for effective (i.e. compute- and cost-efficient) scaling of model size, training data, and total computation. We conduct an extensive empirical study of the performance of large-scale LLM training workloads across model size, hardware configurations, and distributed parallelization strategies. We demonstrate that: (1) beyond certain scales, overhead incurred from certain distributed communication strategies leads parallelization strategies previously thought to be sub-optimal in fact become preferable; and (2) scaling the total number of accelerators for large model training quickly yields diminishing returns even when hardware and parallelization strategies are properly optimized, implying poor marginal performance per additional unit of power or GPU-hour.},
	urldate = {2025-03-30},
	publisher = {arXiv},
	author = {Fernandez, Jared and Wehrstedt, Luca and Shamis, Leonid and Elhoushi, Mostafa and Saladi, Kalyan and Bisk, Yonatan and Strubell, Emma and Kahn, Jacob},
	month = nov,
	year = {2024},
	note = {arXiv:2411.13055 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/henrybaker/Zotero/storage/5R8RJILC/Fernandez et al. - 2024 - Hardware Scaling Trends and Diminishing Returns in.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/MZKIJAKJ/2411.html:text/html},
}

@misc{shi_greenllm_2024,
	title = {{GreenLLM}: {Disaggregating} {Large} {Language} {Model} {Serving} on {Heterogeneous} {GPUs} for {Lower} {Carbon} {Emissions}},
	shorttitle = {{GreenLLM}},
	url = {http://arxiv.org/abs/2412.20322},
	doi = {10.48550/arXiv.2412.20322},
	abstract = {LLMs have been widely adopted across many real-world applications. However, their widespread use comes with significant environmental costs due to their high computational intensity and resource demands. Specifically, this has driven the development of new generations of high-performing GPUs, exacerbating the problem of electronic waste and accelerating the premature disposal of devices. To address this problem, this paper focuses on reducing the carbon emissions of LLM serving by reusing older, low-performing GPUs. We present GreenLLM, an SLO-aware LLM serving framework designed to minimize carbon emissions by reusing older GPUs. GreenLLM builds on two identified use cases that disaggregate specific computations onto older GPUs, reducing carbon emissions while meeting performance goals. To deepen our understanding of the potential carbon savings from disaggregation, we also provide a theoretical analysis of its relationship with carbon intensity and GPU lifetime. Our evaluations show that GreenLLM reduces carbon emissions by up to 40.6\% compared to running standard LLM serving on new GPU only, meeting latency SLOs for over 90\% of requests across various applications, latency requirements, carbon intensities, and GPU lifetimes.},
	urldate = {2025-03-30},
	publisher = {arXiv},
	author = {Shi, Tianyao and Wu, Yanran and Liu, Sihang and Ding, Yi},
	month = dec,
	year = {2024},
	note = {arXiv:2412.20322 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {Preprint PDF:/Users/henrybaker/Zotero/storage/ISP2MU2J/Shi et al. - 2024 - GreenLLM Disaggregating Large Language Model Serv.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/YQPEWT5S/2412.html:text/html},
}

@misc{latotzke_post-training_2022,
	title = {Post-{Training} {Quantization} for {Energy} {Efficient} {Realization} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2210.07906},
	doi = {10.48550/arXiv.2210.07906},
	abstract = {The biggest challenge for the deployment of Deep Neural Networks (DNNs) close to the generated data on edge devices is their size, i.e., memory footprint and computational complexity. Both are significantly reduced with quantization. With the resulting lower word-length, the energy efficiency of DNNs increases proportionally. However, lower word-length typically causes accuracy degradation. To counteract this effect, the quantized DNN is retrained. Unfortunately, training costs up to 5000x more energy than the inference of the quantized DNN. To address this issue, we propose a post-training quantization flow without the need for retraining. For this, we investigated different quantization options. Furthermore, our analysis systematically assesses the impact of reduced word-lengths of weights and activations revealing a clear trend for the choice of word-length. Both aspects have not been systematically investigated so far. Our results are independent of the depth of the DNNs and apply to uniform quantization, allowing fast quantization of a given pre-trained DNN. We excel state-of-the-art for 6 bit by 2.2\% Top-1 accuracy for ImageNet. Without retraining, our quantization to 8 bit surpasses floating-point accuracy.},
	urldate = {2025-04-13},
	publisher = {arXiv},
	author = {Latotzke, Cecilia and Balim, Batuhan and Gemmeke, Tobias},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07906 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/ITSNJKWP/Latotzke et al. - 2022 - Post-Training Quantization for Energy Efficient Re.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/ZWL33E3T/2210.html:text/html},
}

@misc{noauthor_decoding_nodate,
	title = {Decoding {Strategies}: {How} {LLMs} {Choose} {The} {Next} {Word}},
	shorttitle = {Decoding {Strategies}},
	url = {https://assemblyai.com/blog/decoding-strategies-how-llms-choose-the-next-word},
	abstract = {Large Language Models are trained to guess the next word. But when generating text, the combination of their probability estimates with algorithms known as decoding strategies is what determines how they actually choose words. Learn how decoding strategies work in this article.},
	language = {en},
	urldate = {2025-04-13},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/RTIBLT3R/decoding-strategies-how-llms-choose-the-next-word.html:text/html},
}

@misc{openai-developer-community_temperature_2023,
	title = {Temperature, top\_p and top\_k for chatbot responses - {Prompting}},
	url = {https://community.openai.com/t/temperature-top-p-and-top-k-for-chatbot-responses/295542},
	abstract = {Hello  I’m using GPT as a chatbot. I have successfully fine-tuned the model on conversation data. For inference I’m now using temperature = 1, top\_p = 0.6, and top\_k = 35. In the following link it is written that for chatbot responses it is best to use temperature = 0.5 and top\_p = 0.5. On the other hand, I have also read elsewhere that temperature = 1 or top\_p = 1 should hold.  What values for temperature, top\_p and top\_k are best to use for chatbot responses? The chatbot should stick to the le...},
	language = {en},
	urldate = {2025-04-22},
	journal = {OpenAI Developer Community},
	author = {OpenAI-Developer-Community},
	month = jul,
	year = {2023},
	note = {Section: Prompting},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/4PRPAP8Q/295542.html:text/html},
}

@misc{singh_guide_2023,
	title = {A {Guide} to {Controlling} {LLM} {Model} {Output}: {Exploring} {Top}-k, {Top}-p, and {Temperature} {Parameters}},
	shorttitle = {A {Guide} to {Controlling} {LLM} {Model} {Output}},
	url = {https://ivibudh.medium.com/a-guide-to-controlling-llm-model-output-exploring-top-k-top-p-and-temperature-parameters-ed6a31313910},
	abstract = {You might have used ChatGPT or any other major LLM for building a system, doing classification task, answer questions, or using it as an…},
	language = {en},
	urldate = {2025-04-22},
	journal = {Medium},
	author = {Singh, Vibudh},
	month = sep,
	year = {2023},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/IYNU97HB/a-guide-to-controlling-llm-model-output-exploring-top-k-top-p-and-temperature-parameters-ed6a31.html:text/html},
}

@misc{stevens_regulating_2024,
	title = {Regulating {AI}: {The} {Limits} of {FLOPs} as a {Metric}},
	shorttitle = {Regulating {AI}},
	url = {https://medium.com/@ingridwickstevens/regulating-ai-the-limits-of-flops-as-a-metric-41e3b12d5d0c},
	abstract = {An Argument for (if one must) Regulating Applications, Not Math},
	language = {en},
	urldate = {2025-04-24},
	journal = {Medium},
	author = {Stevens, Ingrid},
	month = may,
	year = {2024},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/YGSUMD2U/regulating-ai-the-limits-of-flops-as-a-metric-41e3b12d5d0c.html:text/html},
}

@misc{yao_zeroquant-v2_2023,
	title = {{ZeroQuant}-{V2}: {Exploring} {Post}-training {Quantization} in {LLMs} from {Comprehensive} {Study} to {Low} {Rank} {Compensation}},
	shorttitle = {{ZeroQuant}-{V2}},
	url = {http://arxiv.org/abs/2303.08302},
	doi = {10.48550/arXiv.2303.08302},
	abstract = {Post-training quantization (PTQ) has emerged as a promising technique for mitigating memory consumption and computational costs in large language models (LLMs). However, a systematic examination of various quantization schemes, model families, and quantization bit precision has been absent from the literature. In this paper, we conduct a comprehensive analysis of these factors by investigating the effects of PTQ on weight-only, activation-only, and weight-and-activation quantization using diverse methods such as round-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these methods to two distinct model families with parameters ranging from 125M to 176B. Our contributions include: (1) a sensitivity analysis revealing that activation quantization is generally more susceptible to weight quantization, with smaller models often outperforming larger models in terms of activation quantization; (2) an evaluation and comparison of existing PTQ methods to optimize model size reduction while minimizing the impact on accuracy, revealing that none of the current methods can achieve the original model quality for quantization with either INT4-weight or INT4-weight-and-INT8-activation; (3) based on these insights, we propose an optimized method called Low-Rank Compensation (LoRC), which employs low-rank matrices to enhance model quality recovery with a minimal increase in model size.},
	urldate = {2025-05-08},
	publisher = {arXiv},
	author = {Yao, Zhewei and Wu, Xiaoxia and Li, Cheng and Youn, Stephen and He, Yuxiong},
	month = may,
	year = {2023},
	note = {arXiv:2303.08302 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/65DL3MM8/Yao et al. - 2023 - ZeroQuant-V2 Exploring Post-training Quantization.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/TXIUCA9C/2303.html:text/html},
}

@misc{kakolyris_slo-aware_2024,
	title = {{SLO}-aware {GPU} {Frequency} {Scaling} for {Energy} {Efficient} {LLM} {Inference} {Serving}},
	url = {http://arxiv.org/abs/2408.05235},
	doi = {10.48550/arXiv.2408.05235},
	abstract = {As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present {\textbackslash}textit\{throttLL'eM\}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. {\textbackslash}textit\{throttLL'eM\} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, {\textbackslash}textit\{throttLL'eM\} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves \$R{\textasciicircum}2\$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that {\textbackslash}textit\{throttLL'eM\} achieves up to 43.8{\textbackslash}\% lower energy consumption and an energy efficiency improvement of at least \$1.71{\textbackslash}times\$ under SLOs, when compared to NVIDIA's Triton server.},
	urldate = {2025-05-08},
	publisher = {arXiv},
	author = {Kakolyris, Andreas Kosmas and Masouros, Dimosthenis and Vavaroutsos, Petros and Xydis, Sotirios and Soudris, Dimitrios},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05235 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {Preprint PDF:/Users/henrybaker/Zotero/storage/UDJEVFJN/Kakolyris et al. - 2024 - SLO-aware GPU Frequency Scaling for Energy Efficie.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/9M2QH7EW/2408.html:text/html},
}

@misc{noauthor_proceedings_nodate,
	title = {Proceedings of the {Workshop} on {Design}, {Deployment}, and {Evaluation} of {Network}-assisted {Video} {Streaming}},
	url = {https://dl.acm.org/doi/proceedings/10.1145/3488662},
	language = {en},
	urldate = {2025-05-11},
	journal = {ACM Conferences},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/R6AIXSPU/3488662.html:text/html},
}

@inproceedings{gupta_chasing_2021,
	title = {Chasing {Carbon}: {The} {Elusive} {Environmental} {Footprint} of {Computing}},
	shorttitle = {Chasing {Carbon}},
	url = {https://ieeexplore.ieee.org/document/9407142},
	doi = {10.1109/HPCA51647.2021.00076},
	abstract = {Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems.},
	urldate = {2025-05-11},
	booktitle = {2021 {IEEE} {International} {Symposium} on {High}-{Performance} {Computer} {Architecture} ({HPCA})},
	author = {Gupta, Udit and Kim, Young Geun and Lee, Sylvia and Tse, Jordan and Lee, Hsien-Hsin S. and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},
	month = feb,
	year = {2021},
	note = {ISSN: 2378-203X},
	keywords = {Software, Technological innovation, Carbon dioxide, carbon footprint, Computer architecture, Data center, energy, Energy consumption, Hardware, mobile, Software algorithms},
	pages = {854--867},
	file = {Full Text:/Users/henrybaker/Zotero/storage/LTIXN62G/Gupta et al. - 2021 - Chasing Carbon The Elusive Environmental Footprin.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/R3LLDEGL/9407142.html:text/html},
}

@misc{kwon_efficient_2023,
	title = {Efficient {Memory} {Management} for {Large} {Language} {Model} {Serving} with {PagedAttention}},
	url = {http://arxiv.org/abs/2309.06180},
	doi = {10.48550/arXiv.2309.06180},
	abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4\${\textbackslash}times\$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
	month = sep,
	year = {2023},
	note = {arXiv:2309.06180 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/henrybaker/Zotero/storage/BCTBWPAM/Kwon et al. - 2023 - Efficient Memory Management for Large Language Mod.pdf:application/pdf},
}

@inproceedings{tang_improving_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Improving {Complex} {Knowledge} {Base} {Question} {Answering} via {Question}-to-{Action} and {Question}-to-{Question} {Alignment}},
	url = {https://aclanthology.org/2022.emnlp-main.10/},
	doi = {10.18653/v1/2022.emnlp-main.10},
	abstract = {Complex knowledge base question answering can be achieved by converting questions into sequences of predefined actions. However, there is a significant semantic and structural gap between natural language and action sequences, which makes this conversion difficult. In this paper, we introduce an alignment-enhanced complex question answering framework, called ALCQA, which mitigates this gap through question-to-action alignment and question-to-question alignment. We train a question rewriting model to align the question and each action, and utilize a pretrained language model to implicitly align the question and KG artifacts. Moreover, considering that similar questions correspond to similar action sequences, we retrieve top-k similar question-answer pairs at the inference stage through question-to-question alignment and propose a novel reward-guided action sequence selection strategy to select from candidate action sequences. We conduct experiments on CQA and WQSP datasets, and the results show that our approach outperforms state-of-the-art methods and obtains a 9.88\% improvements in the F1 metric on CQA dataset. Our source code is available at https://github.com/TTTTTTTTy/ALCQA.},
	urldate = {2025-05-11},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Tang, Yechun and Cheng, Xiaoxia and Lu, Weiming},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {137--147},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/W3DU37WL/Tang et al. - 2022 - Improving Complex Knowledge Base Question Answerin.pdf:application/pdf},
}

@article{dettmers_llmint8_2022,
	title = {{LLM}.int8(): 8-bit {Matrix} {Multiplication} for {Transformers} at {Scale}},
	abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open source our software.},
	language = {en},
	author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
	year = {2022},
	file = {Dettmers et al. - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf:/Users/henrybaker/Zotero/storage/HY3LDNKF/Dettmers et al. - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf:application/pdf},
}

@inproceedings{leviathan_fast_2023,
	title = {Fast {Inference} from {Transformers} via {Speculative} {Decoding}},
	url = {https://proceedings.mlr.press/v202/leviathan23a.html},
	abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
	language = {en},
	urldate = {2025-05-11},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {19274--19286},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/VFQDJ8WY/Leviathan et al. - 2023 - Fast Inference from Transformers via Speculative D.pdf:application/pdf},
}

@inproceedings{aminabadi_deepspeed-_2022,
	title = {{DeepSpeed}- {Inference}: {Enabling} {Efficient} {Inference} of {Transformer} {Models} at {Unprecedented} {Scale}},
	shorttitle = {{DeepSpeed}- {Inference}},
	url = {https://ieeexplore.ieee.org/abstract/document/10046087},
	doi = {10.1109/SC41404.2022.00051},
	abstract = {The landscape of transformer model inference is increasingly diverse in model size, model characteristics, latency and throughput requirements, hardware requirements, etc. With such diversity, designing a versatile inference system is challenging. DeepSpeed-Inference addresses these challenges by (1) a multi-GPU inference solution to minimize latency while maximizing throughput for both dense and sparse transformers when the model fits in aggregate GPU memory, and (2) a heterogeneous inference solution that leverages CPU/NVMe/GPU memory to enable high-throughput inference for models larger than aggregate GPU memory. DeepSpeed-Inference reduces latency by 6.4× and increases throughput by 1.5 ×over the state-of-the-art. It enables trillion parameter scale inference under real-time latency constraints by leveraging hundreds of GPUs, an unprecedented scale for inference. It can inference 25 ×larger models than with GPU-only solutions, while delivering a high throughput of 84 TFLOPS (over 50\% of A6000 peak).},
	urldate = {2025-05-11},
	booktitle = {{SC22}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and He, Yuxiong},
	month = nov,
	year = {2022},
	note = {ISSN: 2167-4337},
	keywords = {Deep Learning, Technological innovation, Aggregates, Computational modeling, DeepSpeed, Distributed Inference, Graphics processing units, High performance computing, Mixture of Experts, Production, PyTorch, Transformer models, Transformers},
	pages = {1--15},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/QTIK4VFF/10046087.html:text/html;Submitted Version:/Users/henrybaker/Zotero/storage/2SUMY2LR/Aminabadi et al. - 2022 - DeepSpeed- Inference Enabling Efficient Inference.pdf:application/pdf},
}

@misc{chitty-venkata_llm-inference-bench_2024,
	title = {{LLM}-{Inference}-{Bench}: {Inference} {Benchmarking} of {Large} {Language} {Models} on {AI} {Accelerators}},
	shorttitle = {{LLM}-{Inference}-{Bench}},
	url = {http://arxiv.org/abs/2411.00136},
	doi = {10.48550/arXiv.2411.00136},
	abstract = {Large Language Models (LLMs) have propelled groundbreaking advancements across several domains and are commonly used for text generation applications. However, the computational demands of these complex models pose significant challenges, requiring efficient hardware acceleration. Benchmarking the performance of LLMs across diverse hardware platforms is crucial to understanding their scalability and throughput characteristics. We introduce LLM-Inference-Bench, a comprehensive benchmarking suite to evaluate the hardware inference performance of LLMs. We thoroughly analyze diverse hardware platforms, including GPUs from Nvidia and AMD and specialized AI accelerators, Intel Habana and SambaNova. Our evaluation includes several LLM inference frameworks and models from LLaMA, Mistral, and Qwen families with 7B and 70B parameters. Our benchmarking results reveal the strengths and limitations of various models, hardware platforms, and inference frameworks. We provide an interactive dashboard to help identify configurations for optimal performance for a given hardware platform.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Chitty-Venkata, Krishna Teja and Raskar, Siddhisanket and Kale, Bharat and Ferdaus, Farah and Tanikanti, Aditya and Raffenetti, Ken and Taylor, Valerie and Emani, Murali and Vishwanath, Venkatram},
	month = oct,
	year = {2024},
	note = {arXiv:2411.00136 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/5X3V96TL/Chitty-Venkata et al. - 2024 - LLM-Inference-Bench Inference Benchmarking of Lar.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/ZQSXUSTV/2411.html:text/html},
}

@misc{huggingface_ai_2025,
	title = {{AI} {Energy} {Score}},
	url = {https://huggingface.co/AIEnergyScore},
	abstract = {Org profile for AI Energy Score on Hugging Face, the AI community building the future.},
	urldate = {2025-05-11},
	author = {HuggingFace},
	month = feb,
	year = {2025},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/TB7CYEK3/AIEnergyScore.html:text/html},
}

@misc{huggingface_aienergyscoretext_generation_2024,
	title = {{AIEnergyScore}/text\_generation · {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/AIEnergyScore/text_generation},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-05-11},
	author = {HuggingFace},
	month = dec,
	year = {2024},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/Q48ZQQLQ/text_generation.html:text/html},
}

@misc{noauthor_mlco2codecarbon_2025,
	title = {mlco2/codecarbon},
	copyright = {MIT},
	url = {https://github.com/mlco2/codecarbon},
	abstract = {Track emissions from Compute and recommend ways to reduce their impact on the environment.},
	urldate = {2025-05-11},
	publisher = {CodeCarbon},
	month = may,
	year = {2025},
	note = {original-date: 2020-05-12T14:44:03Z},
}

@article{van_wynsberghe_sustainable_2021,
	title = {Sustainable {AI}: {AI} for sustainability and the sustainability of {AI}},
	volume = {1},
	issn = {2730-5961},
	shorttitle = {Sustainable {AI}},
	url = {https://doi.org/10.1007/s43681-021-00043-6},
	doi = {10.1007/s43681-021-00043-6},
	abstract = {While there is a growing effort towards AI for Sustainability (e.g. towards the sustainable development goals) it is time to move beyond that and to address the sustainability of developing and using AI systems. In this paper I propose a definition of Sustainable AI; Sustainable AI is a movement to foster change in the entire lifecycle of AI products (i.e. idea generation, training, re-tuning, implementation, governance) towards greater ecological integrity and social justice. As such, Sustainable AI is focused on more than AI applications; rather, it addresses the whole sociotechnical system of AI. I have suggested here that Sustainable AI is not about how to sustain the development of AI per say but it is about how to develop AI that is compatible with sustaining environmental resources for current and future generations; economic models for societies; and societal values that are fundamental to a given society. I have articulated that the phrase Sustainable AI be understood as having two branches; AI for sustainability and sustainability of AI (e.g. reduction of carbon emissions and computing power). I propose that Sustainable AI take sustainable development at the core of its definition with three accompanying tensions between AI innovation and equitable resource distribution; inter and intra-generational justice; and, between environment, society, and economy. This paper is not meant to engage with each of the three pillars of sustainability (i.e. social, economic, environment), and as such the pillars of sustainable AI. Rather, this paper is meant to inspire the reader, the policy maker, the AI ethicist, the AI developer to connect with the environment—to remember that there are environmental costs to AI. Further, to direct funding towards sustainable methods of AI.},
	language = {en},
	number = {3},
	urldate = {2025-05-11},
	journal = {AI and Ethics},
	author = {van Wynsberghe, Aimee},
	month = aug,
	year = {2021},
	keywords = {Sustainability, Artificial Intelligence, AI ethics, Responsible AI, Sustainable AI, Sustainable development},
	pages = {213--218},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/HPI34KLL/van Wynsberghe - 2021 - Sustainable AI AI for sustainability and the sust.pdf:application/pdf},
}

@misc{latotzke_post-training_2022-1,
	title = {Post-{Training} {Quantization} for {Energy} {Efficient} {Realization} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2210.07906},
	doi = {10.48550/arXiv.2210.07906},
	abstract = {The biggest challenge for the deployment of Deep Neural Networks (DNNs) close to the generated data on edge devices is their size, i.e., memory footprint and computational complexity. Both are significantly reduced with quantization. With the resulting lower word-length, the energy efficiency of DNNs increases proportionally. However, lower word-length typically causes accuracy degradation. To counteract this effect, the quantized DNN is retrained. Unfortunately, training costs up to 5000x more energy than the inference of the quantized DNN. To address this issue, we propose a post-training quantization flow without the need for retraining. For this, we investigated different quantization options. Furthermore, our analysis systematically assesses the impact of reduced word-lengths of weights and activations revealing a clear trend for the choice of word-length. Both aspects have not been systematically investigated so far. Our results are independent of the depth of the DNNs and apply to uniform quantization, allowing fast quantization of a given pre-trained DNN. We excel state-of-the-art for 6 bit by 2.2\% Top-1 accuracy for ImageNet. Without retraining, our quantization to 8 bit surpasses floating-point accuracy.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Latotzke, Cecilia and Balim, Batuhan and Gemmeke, Tobias},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07906 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/EX4CFTVY/Latotzke et al. - 2022 - Post-Training Quantization for Energy Efficient Re.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/GJLGCZC3/2210.html:text/html},
}

@article{narang_mixed_2018,
	title = {{MIXED} {PRECISION} {TRAINING}},
	abstract = {Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision ﬂoating point numbers, without losing model accuracy or having to modify hyperparameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE halfprecision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to halfprecision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.},
	language = {en},
	author = {Narang, Sharan and Diamos, Gregory and Elsen, Erich and Micikevicius, Paulius and Alben, Jonah and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	year = {2018},
	file = {Narang et al. - 2018 - MIXED PRECISION TRAINING.pdf:/Users/henrybaker/Zotero/storage/DBGRFLA8/Narang et al. - 2018 - MIXED PRECISION TRAINING.pdf:application/pdf},
}

@article{he_campo_2022,
	title = {Campo: {Cost}-{Aware} {Performance} {Optimization} for {Mixed}-{Precision} {Neural} {Network} {Training}},
	abstract = {Mixed precision training uses a mixture of full and lower precisions for neural network (NN) training. Applying mixed precision must cast tensors in NN from float32 (FP32) to float16 (FP16) or vice versa. The existing strategy greedily applies FP16 to performance-critical operations without quantifying and considering the casting cost. However, we reveal that the casting cost can take more than 21\% of NN operation execution time, and in some cases surpasses the performance benefit of using low precision. In this paper, we introduce Campo, a tool that improves performance of mixed-precision NN training with the awareness of casting costs. Campo is built upon performance modeling that predicts the casting cost and operation performance with low precision, and introduces a cost-aware graph rewriting strategy. Campo is user-transparent, and enables high performance NN training using mixed precision without training accuracy loss. Evaluating Campo with six NN models, we show that compared to TensorFlow using TF\_AMP (a state-of-the-art performance optimizer for mixed precision training from Nvidia), Campo improves training throughput by 20.8\% on average (up to 24.5\%) on RTX 2080 Ti GPU and by 20.9\% on average (up to 23.4\%) on V100 GPU, without training accuracy loss. Because of using the cost-aware mixed precision training, Campo also improves energy efficiency by 21.4\% on average (up to 24.2\%), compared to TensorFlow using TF\_AMP.},
	language = {en},
	author = {He, Xin and Sun, Jianhua and Chen, Hao and Li, Dong},
	year = {2022},
	file = {He et al. - Campo Cost-Aware Performance Optimization for Mix.pdf:/Users/henrybaker/Zotero/storage/F6K9JTPI/He et al. - Campo Cost-Aware Performance Optimization for Mix.pdf:application/pdf},
}

@misc{wu_training_2018,
	title = {Training and {Inference} with {Integers} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1802.04680},
	doi = {10.48550/arXiv.1802.04680},
	abstract = {Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as "WAGE" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Wu and Li, Guoqi and Chen, Feng and Shi, Luping},
	month = feb,
	year = {2018},
	note = {arXiv:1802.04680 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/9DQIXQ7Q/Wu et al. - 2018 - Training and Inference with Integers in Deep Neura.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/QTVMRRVY/1802.html:text/html},
}

@inproceedings{gu_trainable_2017,
	address = {Copenhagen, Denmark},
	title = {Trainable {Greedy} {Decoding} for {Neural} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/D17-1210},
	doi = {10.18653/v1/D17-1210},
	abstract = {Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-toend learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any speciﬁc decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to ﬁnd a translation that maximizes an arbitrary decoding objective. More speciﬁcally, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives, and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.},
	language = {en},
	urldate = {2025-05-11},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Gu, Jiatao and Cho, Kyunghyun and Li, Victor O.K.},
	year = {2017},
	pages = {1968--1978},
	file = {Gu et al. - 2017 - Trainable Greedy Decoding for Neural Machine Trans.pdf:/Users/henrybaker/Zotero/storage/ERLH7TX6/Gu et al. - 2017 - Trainable Greedy Decoding for Neural Machine Trans.pdf:application/pdf},
}

@misc{nik_energy-conscious_2025-1,
	title = {Energy-{Conscious} {LLM} {Decoding}: {Impact} of {Text} {Generation} {Strategies} on {GPU} {Energy} {Consumption}},
	shorttitle = {Energy-{Conscious} {LLM} {Decoding}},
	url = {http://arxiv.org/abs/2502.11723},
	doi = {10.48550/arXiv.2502.11723},
	abstract = {Decoding strategies significantly influence the quality and diversity of the generated texts in large language models (LLMs), yet their impact on computational resource consumption, particularly GPU energy usage, is insufficiently studied. This paper investigates the relationship between text generation decoding methods and energy efficiency, focusing on the trade-off between generation quality and GPU energy consumption across diverse tasks and decoding configurations. By benchmarking multiple strategies across different text generation tasks, such as Translation, Code Summarization, and Math Problem Solving, we reveal how selecting appropriate decoding techniques with their tuned hyperparameters affects text quality and has measurable implications for resource utilization, emphasizing the need for balanced optimization. To the best of our knowledge, this study is among the first to explore decoding strategies in LLMs through the lens of energy consumption, offering actionable insights for designing resource-aware applications that maintain high-quality text generation.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Nik, Alireza and Riegler, Michael A. and Halvorsen, Pål},
	month = feb,
	year = {2025},
	note = {arXiv:2502.11723 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/henrybaker/Zotero/storage/M6WIT7KB/Nik et al. - 2025 - Energy-Conscious LLM Decoding Impact of Text Gene.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/LKTWE7UK/2502.html:text/html},
}

@misc{howarth_number_2024,
	title = {Number of {Parameters} in {GPT}-4 ({Latest} {Data})},
	url = {https://explodingtopics.com/blog/gpt-parameters},
	abstract = {An extensive list of statistics covering the number of parameters in ChatGPT-4, ChatGPT-4o, and other AI models.},
	language = {en},
	urldate = {2025-05-14},
	journal = {Exploding Topics},
	author = {Howarth, Josh},
	month = aug,
	year = {2024},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/KHGLFA87/gpt-parameters.html:text/html},
}

@techreport{shehabi_united_2024,
	title = {United {States} {Data} {Center} {Energy} {Usage} {Report}},
	url = {http://www.osti.gov/servlets/purl/1372902/},
	language = {en},
	number = {LBNL--1005775, 1372902},
	urldate = {2025-05-16},
	author = {Shehabi, Arman and Smith, Sarah and Sartor, Dale and Brown, Richard and Herrlin, Magnus and Koomey, Jonathan and Masanet, Eric and Horner, Nathaniel and Azevedo, Inês and Lintner, William},
	year = {2024},
	doi = {10.2172/1372902},
	pages = {LBNL--1005775, 1372902},
	file = {Shehabi et al. - 2016 - United States Data Center Energy Usage Report.pdf:/Users/henrybaker/Zotero/storage/HN6W84RS/Shehabi et al. - 2016 - United States Data Center Energy Usage Report.pdf:application/pdf},
}

@misc{amazon_amazon_2019,
	title = {Amazon {EC2} {Update}},
	shorttitle = {– {Inf1} {Instances} with {AWS} {Inferentia} {Chips} for {High} {Performance} {Cost}-{Effective} {Inferencing} {\textbar} {AWS} {News} {Blog}},
	url = {https://aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/},
	language = {en-US},
	urldate = {2025-05-16},
	author = {Amazon},
	month = dec,
	year = {2019},
	note = {Section: Amazon EC2},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/GSWNZP6F/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-.html:text/html},
}

@misc{noauthor_ai_2025,
	title = {{AI} is set to drive surging electricity demand from data centres while offering the potential to transform how the energy sector works - {News}},
	url = {https://www.iea.org/news/ai-is-set-to-drive-surging-electricity-demand-from-data-centres-while-offering-the-potential-to-transform-how-the-energy-sector-works},
	abstract = {AI is set to drive surging electricity demand from data centres while offering the potential to transform how the energy sector works - News from the International Energy Agency},
	language = {en-GB},
	urldate = {2025-05-16},
	journal = {IEA},
	month = apr,
	year = {2025},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/P8TRX4NG/ai-is-set-to-drive-surging-electricity-demand-from-data-centres-while-offering-the-potential-to.html:text/html},
}

@misc{holtzman_curious_2020,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {http://arxiv.org/abs/1904.09751},
	doi = {10.48550/arXiv.1904.09751},
	abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
	urldate = {2025-05-16},
	publisher = {arXiv},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09751 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/henrybaker/Zotero/storage/YXRUT9WM/Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf:application/pdf;Snapshot:/Users/henrybaker/Zotero/storage/YSY483IL/1904.html:text/html},
}

@misc{noauthor_exclusive_2024,
	title = {Exclusive: {Uncovering} the {Real} {Battery} {Capacities} of the {iPhone} 16 {Series}},
	shorttitle = {Exclusive},
	url = {https://blogdoiphone.com/en/news/exclusive-uncovering-the-real-battery-capacities-of-the-iphone-16-series/},
	abstract = {Explore all battery capacities of the iPhone 16 series, revealed through brazilian regulatory agency, and see how they compare to the iPhone 15 models.},
	language = {en-GB},
	urldate = {2025-05-17},
	month = sep,
	year = {2024},
}

@misc{noauthor_macbook_nodate,
	title = {{MacBook} {Pro} - {Tech} {Specs}},
	url = {https://www.apple.com/macbook-pro/specs/},
	abstract = {See all the technical specifications for the 14- and 16-inch MacBook Pro with the M4, M4 Pro, and M4 Max chips.},
	language = {en-US},
	urldate = {2025-05-17},
	journal = {Apple},
}

@misc{noauthor_electricity_nodate,
	title = {Electricity usage of a {Wi}-{Fi} {Router} - {Energy} {Use} {Calculator}},
	url = {https://energyusecalculator.com/electricity_wifirouter.htm},
	urldate = {2025-05-17},
	file = {Electricity usage of a Wi-Fi Router - Energy Use Calculator:/Users/henrybaker/Zotero/storage/6HDTA356/electricity_wifirouter.html:text/html},
}

@misc{noauthor_carbon_2020,
	title = {The carbon footprint of streaming video: fact-checking the headlines – {Analysis}},
	shorttitle = {The carbon footprint of streaming video},
	url = {https://www.iea.org/commentaries/the-carbon-footprint-of-streaming-video-fact-checking-the-headlines},
	abstract = {The carbon footprint of streaming video: fact-checking the headlines - A commentary by George Kamiya},
	language = {en-GB},
	urldate = {2025-05-17},
	journal = {IEA},
	month = dec,
	year = {2020},
}

@misc{rw5603_how_2024,
	title = {How {Much} {Energy} {Do} {Google} {Search} and {ChatGPT} {Use}?},
	url = {https://www.rwdigital.ca/blog/how-much-energy-do-google-search-and-chatgpt-use/},
	abstract = {Explore the surprising energy and environmental impact of every Google search and ChatGPT query in our digital age.},
	language = {en-US},
	urldate = {2025-05-17},
	journal = {RW Digital - Vancouver Digital Marketing Agency},
	author = {rw5603},
	month = oct,
	year = {2024},
}

@misc{noauthor_kettle_nodate,
	title = {Kettle - {Wikiwand}},
	url = {https://www.wikiwand.com/en/articles/Kettle},
	abstract = {A kettle, sometimes called a tea kettle or teakettle, is a device specialized for boiling water, commonly with a lid, spout, and handle. There are two main type...},
	language = {en},
	urldate = {2025-05-17},
}

@misc{noauthor_understanding_nodate,
	title = {Understanding {Electric} {Shower} kilowatt ({kW}) {Ratings} by {Mira} {Showers}},
	url = {https://www.mirashowers.co.uk/blog/mira-recommends/kw-ratings-explained},
	abstract = {Read our practical guide to understand the difference between 7.5kW, 8.5kW, 9.5kW \& 10.5kW ratings for electric showers.},
	language = {en-GB},
	urldate = {2025-05-17},
	file = {Snapshot:/Users/henrybaker/Zotero/storage/CGKAJBRL/kw-ratings-explained.html:text/html},
}

@misc{iea_energy_2025,
	title = {Energy and {AI} – {Analysis}},
	url = {https://www.iea.org/reports/energy-and-ai},
	urldate = {2025-05-20},
	author = {IEA},
	year = {2025},
	file = {Energy and AI – Analysis - IEA:/Users/henrybaker/Zotero/storage/77BTVW2T/energy-and-ai.html:text/html},
}
