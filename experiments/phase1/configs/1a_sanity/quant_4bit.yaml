# Phase 1a: 4-bit Quantization
config_name: sanity_quant_4bit
model_name: meta-llama/Llama-3.2-1B

max_input_tokens: 128
max_output_tokens: 128
num_input_prompts: 32

gpus: [0]
num_processes: 1
num_cycles: 2

fp_precision: float16

batching:
  batch_size: 8
  strategy: static

decoder:
  temperature: 0.8
  do_sample: true
  top_k: 50
  top_p: 1.0

quantization:
  quantization: true
  load_in_8bit: false
  load_in_4bit: true
  bnb_4bit_compute_dtype: float16
  bnb_4bit_quant_type: nf4

prompts:
  type: huggingface
  dataset: ai-energy-score
  split: train
  sample_size: 32
