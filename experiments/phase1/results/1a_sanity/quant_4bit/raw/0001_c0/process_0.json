{
  "schema_version": "2.0.0",
  "experiment_id": "0001_c0",
  "backend": "pytorch",
  "backend_version": "transformers=4.57.6, torch=2.5.1+cu124",
  "process_index": 0,
  "gpu_id": 0,
  "gpu_name": "NVIDIA A100-PCIE-40GB",
  "gpu_is_mig": false,
  "gpu_mig_profile": null,
  "energy_measurement_warning": null,
  "config_name": "sanity_quant_4bit",
  "model_name": "meta-llama/Llama-3.2-1B",
  "timestamps": {
    "start": "2026-01-16T19:49:10.352392",
    "end": "2026-01-16T19:49:56.136266",
    "duration_sec": 45.783874
  },
  "inference_metrics": {
    "total_tokens": 8192,
    "input_tokens": 4096,
    "output_tokens": 4096,
    "inference_time_sec": 13.296079981140792,
    "tokens_per_second": 616.1214441865243,
    "latency_per_token_ms": 1.6230566383228506,
    "time_to_first_token_ms": null,
    "latency_measurements": null
  },
  "energy_metrics": {
    "total_energy_j": 3112.6468646400635,
    "gpu_energy_j": 1831.7394653877273,
    "cpu_energy_j": 1268.9384281499301,
    "ram_energy_j": 11.968971102406112,
    "gpu_power_w": 187.1232681345784,
    "cpu_power_w": 99.04670057825957,
    "duration_sec": 0.0,
    "emissions_kg_co2": 2.0554005998425094e-6,
    "energy_per_token_j": 0.0
  },
  "compute_metrics": {
    "flops_total": 12276123828224.0,
    "flops_per_token": 1498550272.0,
    "flops_per_second": 923288957770.7488,
    "peak_memory_mb": 0.0,
    "model_memory_mb": 0.0,
    "flops_method": "parameter_estimate",
    "flops_confidence": "low",
    "compute_precision": "float16"
  },
  "effective_config": {
    "adapter": null,
    "backend": "pytorch",
    "batching_options": {
      "batch_size": 8,
      "dynamic_batching": false,
      "max_tokens_per_batch": null,
      "strategy": "static"
    },
    "config_name": "sanity_quant_4bit",
    "cycle_id": null,
    "decode_token_to_text": false,
    "decoder_config": {
      "do_sample": true,
      "min_p": 0.0,
      "no_repeat_ngram_size": 0,
      "preset": null,
      "repetition_penalty": 1.0,
      "temperature": 0.8,
      "top_k": 50,
      "top_p": 1.0
    },
    "extra_metadata": {},
    "fp_precision": "float16",
    "gpu_list": [
      0
    ],
    "inference_type": "pure_generative",
    "io_config": {
      "results_dir": null
    },
    "is_encoder_decoder": false,
    "latency_simulation": {
      "enabled": false,
      "mode": "poisson",
      "seed": null,
      "target_qps": 1.0
    },
    "max_input_tokens": 128,
    "max_output_tokens": 128,
    "min_output_tokens": 0,
    "model_name": "meta-llama/Llama-3.2-1B",
    "num_cycles": 2,
    "num_input_prompts": 32,
    "num_processes": 1,
    "prompt_source": {
      "column": "text",
      "dataset": "AIEnergyScore/text_generation",
      "sample_size": 32,
      "seed": 42,
      "shuffle": false,
      "split": "train",
      "subset": null,
      "type": "huggingface"
    },
    "pytorch": null,
    "quantization_config": {
      "bnb_4bit_compute_dtype": "float16",
      "bnb_4bit_quant_type": "nf4",
      "bnb_4bit_use_double_quant": false,
      "load_in_4bit": true,
      "load_in_8bit": false,
      "quantization": true
    },
    "query_rate": 1.0,
    "random_seed": null,
    "save_outputs": false,
    "schedule_config": {
      "at": null,
      "days": null,
      "enabled": false,
      "interval": null,
      "total_duration": "24h"
    },
    "sharding_config": {
      "num_shards": 1,
      "strategy": "none",
      "tp_plan": null
    },
    "streaming": false,
    "streaming_warmup_requests": 5,
    "task_type": "text_generation",
    "tensorrt": null,
    "vllm": null
  },
  "cli_overrides": {},
  "config_warnings": []
}