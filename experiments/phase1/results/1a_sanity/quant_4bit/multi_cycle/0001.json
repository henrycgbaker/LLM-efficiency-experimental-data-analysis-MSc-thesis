{
  "schema_version": "2.0.0",
  "experiment_id": "0001",
  "backend": "pytorch",
  "backend_version": null,
  "num_cycles": 2,
  "statistics": {
    "num_cycles": 2,
    "energy_mean_j": 3155.7638538407577,
    "energy_std_j": 60.976630896316024,
    "energy_ci_95_lower": 2607.919389056737,
    "energy_ci_95_upper": 3703.6083186247783,
    "throughput_mean_tps": 617.3376544480532,
    "throughput_std_tps": 1.7199810465514793,
    "throughput_ci_95_lower": 601.8844868650672,
    "throughput_ci_95_upper": 632.7908220310392,
    "efficiency_mean_tpj": 2.596369662811747,
    "efficiency_std_tpj": 0.05016784586304909,
    "latency_mean_ms": 1.6198653590322465,
    "latency_std_ms": 0.004513150454092612,
    "energy_cv": 0.019322304747899224,
    "throughput_cv": 0.0027861269017994267
  },
  "cycle_results": [
    {
      "schema_version": "2.0.0",
      "experiment_id": "0001_c0",
      "backend": "pytorch",
      "backend_version": "transformers=4.57.6, torch=2.5.1+cu124",
      "aggregation": {
        "method": "sum_energy_avg_throughput",
        "num_processes": 1,
        "temporal_overlap_verified": false,
        "gpu_attribution_verified": true,
        "warnings": []
      },
      "total_tokens": 8192,
      "total_energy_j": 3112.6468646400635,
      "total_inference_time_sec": 13.296079981140792,
      "avg_tokens_per_second": 616.1214441865243,
      "avg_energy_per_token_j": 0.37996177546875776,
      "total_flops": 12276123828224.0,
      "process_results": [
        {
          "schema_version": "2.0.0",
          "experiment_id": "0001_c0",
          "backend": "pytorch",
          "backend_version": "transformers=4.57.6, torch=2.5.1+cu124",
          "process_index": 0,
          "gpu_id": 0,
          "gpu_name": "NVIDIA A100-PCIE-40GB",
          "gpu_is_mig": false,
          "gpu_mig_profile": null,
          "energy_measurement_warning": null,
          "config_name": "sanity_quant_4bit",
          "model_name": "meta-llama/Llama-3.2-1B",
          "timestamps": {
            "start": "2026-01-16T19:49:10.352392",
            "end": "2026-01-16T19:49:56.136266",
            "duration_sec": 45.783874
          },
          "inference_metrics": {
            "total_tokens": 8192,
            "input_tokens": 4096,
            "output_tokens": 4096,
            "inference_time_sec": 13.296079981140792,
            "tokens_per_second": 616.1214441865243,
            "latency_per_token_ms": 1.6230566383228506,
            "time_to_first_token_ms": null,
            "latency_measurements": null
          },
          "energy_metrics": {
            "total_energy_j": 3112.6468646400635,
            "gpu_energy_j": 1831.7394653877273,
            "cpu_energy_j": 1268.9384281499301,
            "ram_energy_j": 11.968971102406112,
            "gpu_power_w": 187.1232681345784,
            "cpu_power_w": 99.04670057825957,
            "duration_sec": 0.0,
            "emissions_kg_co2": 2.0554005998425094e-6,
            "energy_per_token_j": 0.0
          },
          "compute_metrics": {
            "flops_total": 12276123828224.0,
            "flops_per_token": 1498550272.0,
            "flops_per_second": 923288957770.7488,
            "peak_memory_mb": 0.0,
            "model_memory_mb": 0.0,
            "flops_method": "parameter_estimate",
            "flops_confidence": "low",
            "compute_precision": "float16"
          },
          "effective_config": {
            "adapter": null,
            "backend": "pytorch",
            "batching_options": {
              "batch_size": 8,
              "dynamic_batching": false,
              "max_tokens_per_batch": null,
              "strategy": "static"
            },
            "config_name": "sanity_quant_4bit",
            "cycle_id": null,
            "decode_token_to_text": false,
            "decoder_config": {
              "do_sample": true,
              "min_p": 0.0,
              "no_repeat_ngram_size": 0,
              "preset": null,
              "repetition_penalty": 1.0,
              "temperature": 0.8,
              "top_k": 50,
              "top_p": 1.0
            },
            "extra_metadata": {},
            "fp_precision": "float16",
            "gpu_list": [
              0
            ],
            "inference_type": "pure_generative",
            "io_config": {
              "results_dir": null
            },
            "is_encoder_decoder": false,
            "latency_simulation": {
              "enabled": false,
              "mode": "poisson",
              "seed": null,
              "target_qps": 1.0
            },
            "max_input_tokens": 128,
            "max_output_tokens": 128,
            "min_output_tokens": 0,
            "model_name": "meta-llama/Llama-3.2-1B",
            "num_cycles": 2,
            "num_input_prompts": 32,
            "num_processes": 1,
            "prompt_source": {
              "column": "text",
              "dataset": "AIEnergyScore/text_generation",
              "sample_size": 32,
              "seed": 42,
              "shuffle": false,
              "split": "train",
              "subset": null,
              "type": "huggingface"
            },
            "pytorch": null,
            "quantization_config": {
              "bnb_4bit_compute_dtype": "float16",
              "bnb_4bit_quant_type": "nf4",
              "bnb_4bit_use_double_quant": false,
              "load_in_4bit": true,
              "load_in_8bit": false,
              "quantization": true
            },
            "query_rate": 1.0,
            "random_seed": null,
            "save_outputs": false,
            "schedule_config": {
              "at": null,
              "days": null,
              "enabled": false,
              "interval": null,
              "total_duration": "24h"
            },
            "sharding_config": {
              "num_shards": 1,
              "strategy": "none",
              "tp_plan": null
            },
            "streaming": false,
            "streaming_warmup_requests": 5,
            "task_type": "text_generation",
            "tensorrt": null,
            "vllm": null
          },
          "cli_overrides": {},
          "config_warnings": []
        }
      ],
      "start_time": "2026-01-16T19:49:10.352392",
      "end_time": "2026-01-16T19:49:56.136266",
      "effective_config": {
        "adapter": null,
        "backend": "pytorch",
        "batching_options": {
          "batch_size": 8,
          "dynamic_batching": false,
          "max_tokens_per_batch": null,
          "strategy": "static"
        },
        "config_name": "sanity_quant_4bit",
        "cycle_id": null,
        "decode_token_to_text": false,
        "decoder_config": {
          "do_sample": true,
          "min_p": 0.0,
          "no_repeat_ngram_size": 0,
          "preset": null,
          "repetition_penalty": 1.0,
          "temperature": 0.8,
          "top_k": 50,
          "top_p": 1.0
        },
        "extra_metadata": {},
        "fp_precision": "float16",
        "gpu_list": [
          0
        ],
        "inference_type": "pure_generative",
        "io_config": {
          "results_dir": null
        },
        "is_encoder_decoder": false,
        "latency_simulation": {
          "enabled": false,
          "mode": "poisson",
          "seed": null,
          "target_qps": 1.0
        },
        "max_input_tokens": 128,
        "max_output_tokens": 128,
        "min_output_tokens": 0,
        "model_name": "meta-llama/Llama-3.2-1B",
        "num_cycles": 2,
        "num_input_prompts": 32,
        "num_processes": 1,
        "prompt_source": {
          "column": "text",
          "dataset": "AIEnergyScore/text_generation",
          "sample_size": 32,
          "seed": 42,
          "shuffle": false,
          "split": "train",
          "subset": null,
          "type": "huggingface"
        },
        "pytorch": null,
        "quantization_config": {
          "bnb_4bit_compute_dtype": "float16",
          "bnb_4bit_quant_type": "nf4",
          "bnb_4bit_use_double_quant": false,
          "load_in_4bit": true,
          "load_in_8bit": false,
          "quantization": true
        },
        "query_rate": 1.0,
        "random_seed": null,
        "save_outputs": false,
        "schedule_config": {
          "at": null,
          "days": null,
          "enabled": false,
          "interval": null,
          "total_duration": "24h"
        },
        "sharding_config": {
          "num_shards": 1,
          "strategy": "none",
          "tp_plan": null
        },
        "streaming": false,
        "streaming_warmup_requests": 5,
        "task_type": "text_generation",
        "tensorrt": null,
        "vllm": null
      },
      "cli_overrides": {},
      "config_warnings": [],
      "latency_stats": null
    },
    {
      "schema_version": "2.0.0",
      "experiment_id": "0001_c1",
      "backend": "pytorch",
      "backend_version": "transformers=4.57.6, torch=2.5.1+cu124",
      "aggregation": {
        "method": "sum_energy_avg_throughput",
        "num_processes": 1,
        "temporal_overlap_verified": false,
        "gpu_attribution_verified": true,
        "warnings": []
      },
      "total_tokens": 8192,
      "total_energy_j": 3198.880843041452,
      "total_inference_time_sec": 13.243794061243534,
      "avg_tokens_per_second": 618.5538647095821,
      "avg_energy_per_token_j": 0.3904883841603335,
      "total_flops": 12276123828224.0,
      "process_results": [
        {
          "schema_version": "2.0.0",
          "experiment_id": "0001_c1",
          "backend": "pytorch",
          "backend_version": "transformers=4.57.6, torch=2.5.1+cu124",
          "process_index": 0,
          "gpu_id": 0,
          "gpu_name": "NVIDIA A100-PCIE-40GB",
          "gpu_is_mig": false,
          "gpu_mig_profile": null,
          "energy_measurement_warning": null,
          "config_name": "sanity_quant_4bit",
          "model_name": "meta-llama/Llama-3.2-1B",
          "timestamps": {
            "start": "2026-01-16T19:50:12.898221",
            "end": "2026-01-16T19:50:35.593794",
            "duration_sec": 22.695573
          },
          "inference_metrics": {
            "total_tokens": 8192,
            "input_tokens": 4096,
            "output_tokens": 4096,
            "inference_time_sec": 13.243794061243534,
            "tokens_per_second": 618.5538647095821,
            "latency_per_token_ms": 1.6166740797416423,
            "time_to_first_token_ms": null,
            "latency_measurements": null
          },
          "energy_metrics": {
            "total_energy_j": 3198.880843041452,
            "gpu_energy_j": 1835.0594680413224,
            "cpu_energy_j": 1355.540403431455,
            "ram_energy_j": 8.280971568674948,
            "gpu_power_w": 166.87821412034322,
            "cpu_power_w": 102.08885625698184,
            "duration_sec": 0.0,
            "emissions_kg_co2": 0.0003385037936546226,
            "energy_per_token_j": 0.0
          },
          "compute_metrics": {
            "flops_total": 12276123828224.0,
            "flops_per_token": 1498550272.0,
            "flops_per_second": 926934062207.1954,
            "peak_memory_mb": 0.0,
            "model_memory_mb": 0.0,
            "flops_method": "parameter_estimate",
            "flops_confidence": "low",
            "compute_precision": "float16"
          },
          "effective_config": {
            "adapter": null,
            "backend": "pytorch",
            "batching_options": {
              "batch_size": 8,
              "dynamic_batching": false,
              "max_tokens_per_batch": null,
              "strategy": "static"
            },
            "config_name": "sanity_quant_4bit",
            "cycle_id": null,
            "decode_token_to_text": false,
            "decoder_config": {
              "do_sample": true,
              "min_p": 0.0,
              "no_repeat_ngram_size": 0,
              "preset": null,
              "repetition_penalty": 1.0,
              "temperature": 0.8,
              "top_k": 50,
              "top_p": 1.0
            },
            "extra_metadata": {},
            "fp_precision": "float16",
            "gpu_list": [
              0
            ],
            "inference_type": "pure_generative",
            "io_config": {
              "results_dir": null
            },
            "is_encoder_decoder": false,
            "latency_simulation": {
              "enabled": false,
              "mode": "poisson",
              "seed": null,
              "target_qps": 1.0
            },
            "max_input_tokens": 128,
            "max_output_tokens": 128,
            "min_output_tokens": 0,
            "model_name": "meta-llama/Llama-3.2-1B",
            "num_cycles": 2,
            "num_input_prompts": 32,
            "num_processes": 1,
            "prompt_source": {
              "column": "text",
              "dataset": "AIEnergyScore/text_generation",
              "sample_size": 32,
              "seed": 42,
              "shuffle": false,
              "split": "train",
              "subset": null,
              "type": "huggingface"
            },
            "pytorch": null,
            "quantization_config": {
              "bnb_4bit_compute_dtype": "float16",
              "bnb_4bit_quant_type": "nf4",
              "bnb_4bit_use_double_quant": false,
              "load_in_4bit": true,
              "load_in_8bit": false,
              "quantization": true
            },
            "query_rate": 1.0,
            "random_seed": null,
            "save_outputs": false,
            "schedule_config": {
              "at": null,
              "days": null,
              "enabled": false,
              "interval": null,
              "total_duration": "24h"
            },
            "sharding_config": {
              "num_shards": 1,
              "strategy": "none",
              "tp_plan": null
            },
            "streaming": false,
            "streaming_warmup_requests": 5,
            "task_type": "text_generation",
            "tensorrt": null,
            "vllm": null
          },
          "cli_overrides": {},
          "config_warnings": []
        }
      ],
      "start_time": "2026-01-16T19:50:12.898221",
      "end_time": "2026-01-16T19:50:35.593794",
      "effective_config": {
        "adapter": null,
        "backend": "pytorch",
        "batching_options": {
          "batch_size": 8,
          "dynamic_batching": false,
          "max_tokens_per_batch": null,
          "strategy": "static"
        },
        "config_name": "sanity_quant_4bit",
        "cycle_id": null,
        "decode_token_to_text": false,
        "decoder_config": {
          "do_sample": true,
          "min_p": 0.0,
          "no_repeat_ngram_size": 0,
          "preset": null,
          "repetition_penalty": 1.0,
          "temperature": 0.8,
          "top_k": 50,
          "top_p": 1.0
        },
        "extra_metadata": {},
        "fp_precision": "float16",
        "gpu_list": [
          0
        ],
        "inference_type": "pure_generative",
        "io_config": {
          "results_dir": null
        },
        "is_encoder_decoder": false,
        "latency_simulation": {
          "enabled": false,
          "mode": "poisson",
          "seed": null,
          "target_qps": 1.0
        },
        "max_input_tokens": 128,
        "max_output_tokens": 128,
        "min_output_tokens": 0,
        "model_name": "meta-llama/Llama-3.2-1B",
        "num_cycles": 2,
        "num_input_prompts": 32,
        "num_processes": 1,
        "prompt_source": {
          "column": "text",
          "dataset": "AIEnergyScore/text_generation",
          "sample_size": 32,
          "seed": 42,
          "shuffle": false,
          "split": "train",
          "subset": null,
          "type": "huggingface"
        },
        "pytorch": null,
        "quantization_config": {
          "bnb_4bit_compute_dtype": "float16",
          "bnb_4bit_quant_type": "nf4",
          "bnb_4bit_use_double_quant": false,
          "load_in_4bit": true,
          "load_in_8bit": false,
          "quantization": true
        },
        "query_rate": 1.0,
        "random_seed": null,
        "save_outputs": false,
        "schedule_config": {
          "at": null,
          "days": null,
          "enabled": false,
          "interval": null,
          "total_duration": "24h"
        },
        "sharding_config": {
          "num_shards": 1,
          "strategy": "none",
          "tp_plan": null
        },
        "streaming": false,
        "streaming_warmup_requests": 5,
        "task_type": "text_generation",
        "tensorrt": null,
        "vllm": null
      },
      "cli_overrides": {},
      "config_warnings": [],
      "latency_stats": null
    }
  ],
  "cycle_metadata": [
    {
      "cycle_id": 0,
      "timestamp": "2026-01-16T19:48:57.039816",
      "gpu_temperature_c": 31.0,
      "system_load": 5.04052734375
    },
    {
      "cycle_id": 1,
      "timestamp": "2026-01-16T19:49:57.542742",
      "gpu_temperature_c": 33.0,
      "system_load": 4.7138671875
    }
  ],
  "start_time": "2026-01-16T19:49:10.352392",
  "end_time": "2026-01-16T19:50:35.593794",
  "total_duration_sec": 85.241402,
  "effective_config": {
    "config_name": "sanity_quant_4bit",
    "model_name": "meta-llama/Llama-3.2-1B",
    "adapter": null,
    "is_encoder_decoder": false,
    "task_type": "text_generation",
    "inference_type": "pure_generative",
    "max_input_tokens": 128,
    "max_output_tokens": 128,
    "min_output_tokens": 0,
    "num_input_prompts": 32,
    "save_outputs": false,
    "decode_token_to_text": false,
    "prompt_source": {
      "type": "huggingface",
      "dataset": "AIEnergyScore/text_generation",
      "split": "train",
      "subset": null,
      "column": "text",
      "sample_size": 32,
      "shuffle": false,
      "seed": 42
    },
    "gpu_list": [
      0
    ],
    "num_processes": 1,
    "batching_options": {
      "batch_size": 8,
      "strategy": "static",
      "max_tokens_per_batch": null,
      "dynamic_batching": false
    },
    "sharding_config": {
      "strategy": "none",
      "num_shards": 1,
      "tp_plan": null
    },
    "latency_simulation": {
      "enabled": false,
      "mode": "poisson",
      "target_qps": 1.0,
      "seed": null
    },
    "decoder_config": {
      "temperature": 0.8,
      "do_sample": true,
      "top_p": 1.0,
      "top_k": 50,
      "min_p": 0.0,
      "repetition_penalty": 1.0,
      "no_repeat_ngram_size": 0,
      "preset": null
    },
    "quantization_config": {
      "quantization": true,
      "load_in_4bit": true,
      "load_in_8bit": false,
      "bnb_4bit_compute_dtype": "float16",
      "bnb_4bit_quant_type": "nf4",
      "bnb_4bit_use_double_quant": false
    },
    "schedule_config": {
      "enabled": false,
      "interval": null,
      "at": null,
      "days": null,
      "total_duration": "24h"
    },
    "io_config": {
      "results_dir": null
    },
    "fp_precision": "float16",
    "backend": "pytorch",
    "streaming": false,
    "streaming_warmup_requests": 5,
    "vllm": null,
    "pytorch": null,
    "tensorrt": null,
    "cycle_id": null,
    "num_cycles": 2,
    "query_rate": 1.0,
    "random_seed": null,
    "extra_metadata": {}
  },
  "config_warnings": []
}